<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Flask on xvnpw personal blog</title><link>https://xvnpw.github.io/tags/flask/</link><description>Recent content in Flask on xvnpw personal blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 02 Feb 2025 09:00:00 +0100</lastBuildDate><atom:link href="https://xvnpw.github.io/tags/flask/index.xml" rel="self" type="application/rss+xml"/><item><title>Forget Threats, Mitigations are All You REALLY Need</title><link>https://xvnpw.github.io/posts/forget-threats-mitigations-are-all-you-really-need/</link><pubDate>Sun, 02 Feb 2025 09:00:00 +0100</pubDate><guid>https://xvnpw.github.io/posts/forget-threats-mitigations-are-all-you-really-need/</guid><description>&lt;p>Ever feel like you&amp;rsquo;re speaking a different language when you talk to developers about security? You&amp;rsquo;re buzzing with threat models, attack vectors, and vulnerabilities, while they&amp;rsquo;re focused on features, deadlines, and, well, &lt;em>making things work&lt;/em>. I get it. I&amp;rsquo;ve been there. And recently, I had a bit of an &amp;ldquo;aha!&amp;rdquo; moment that completely shifted my perspective on security discussions.&lt;/p>
&lt;p>It happened during a review of security analysis with my colleagues at Form3. We were diving deep into the &lt;a href="https://github.com/form3tech-oss/terraform-provider-chronicle">terraform-provider-chronicle&lt;/a> project (you can see analysis &lt;a href="https://github.com/xvnpw/ai-security-analyzer/tree/main/examples/form3tech-oss/README.md">here&lt;/a>, using my &lt;a href="https://github.com/xvnpw/ai-security-analyzer">ai-security-analyzer&lt;/a> tool). As we talked through AI generated threats, it struck me: to make AI tools useful for developers, we need to focus on mitigations, not threats. Developers are less interested in abstract threats and far more engaged when you talk about &lt;strong>mitigations&lt;/strong> â€“ concrete steps to &lt;em>fix&lt;/em> things.&lt;/p></description></item><item><title>Deep Analysis Mode in AI Security Analyzer</title><link>https://xvnpw.github.io/posts/ai-security-analyzer-deep-analysis-mode/</link><pubDate>Fri, 10 Jan 2025 20:00:00 +0100</pubDate><guid>https://xvnpw.github.io/posts/ai-security-analyzer-deep-analysis-mode/</guid><description>&lt;p>First off, &lt;strong>a big thank you&lt;/strong> ðŸ™‡ to everyone who provided such positive feedback on my previous post, &lt;a href="https://xvnpw.github.io/posts/scaling-threat-modeling-with-ai/">Scaling Threat Modeling with AI: Generating 1000 Threat Models Using Gemini 2.0 and AI Security Analyzer&lt;/a>. Your insights and suggestions have been incredibly valuable.&lt;/p>
&lt;p>Inspired by your comments, I&amp;rsquo;ve added a new feature to the AI Security Analyzer: &lt;strong>Deep Analysis Mode&lt;/strong>. In this post, I&amp;rsquo;ll walk you through how it works and showcase its capabilities using Google&amp;rsquo;s Gemini 2.0 Flash Thinking Experimental model to perform an in-depth threat modeling on the &lt;a href="https://github.com/pallets/flask">Flask&lt;/a> project. We&amp;rsquo;ll compare outputs between Normal Mode and Deep Analysis Mode to highlight the differences.&lt;/p></description></item></channel></rss>