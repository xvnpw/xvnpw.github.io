---
title: "Sonnet 3.7 and New Vulnerability Workflow"
date: 2025-03-01T16:00:00+01:00
draft: true
tags: [security, ai, vulnerabilities, screenshot-to-code, github, llm]
description: "Showcasing a new vulnerability workflow using Sonnet 3.7 and a custom prompt."
---

With release new model from Anthropic, Sonnet 3.7, I've updated my [ai-security-analyzer](https://github.com/xvnpw/ai-security-analyzer) to use the new model. I also tested it as same benchmark as other models in my previous [post]({{< ref "posts/can-ai-actually-find-real-security-bugs-testing-the-new-wave-of-ai-models.md" >}}).

## How to use Sonnet 3.7

To use Sonnet 3.7, you can run the following command:


```bash
python ai_security_analyzer/app.py \
    dir \
    -t ../path/to/project \
    -o vulnerabilities.md \
    --agent-prompt-type vulnerabilities \
    --agent-provider openrouter \ # or anthropic
    --agent-model anthropic/claude-3.7-sonnet:thinking \ # or claude-3.7-sonnet-latest
    --agent-temperature 1
```

## The Experiment: Testing Against Real Code

### Methodology

I'll run the [prompt](https://github.com/xvnpw/ai-security-analyzer/blob/main/ai_security_analyzer/prompts/default/dir/vulnerabilities-default.yaml) against a project I'm familiar with: [screenshot-to-code](https://github.com/abi/screenshot-to-code). I'll then manually verify each reported vulnerability by attempting to reproduce it. This will give us a sense of the true positive and false positive rates model.

## Results and Analysis

Here's a summary of the results for each model. I've included detailed outputs in the `ai-security-analyzer` repository.

### Anthropic Sonnet 3.7 Thinking

Detailed output: [vulnerabilities.md](https://github.com/xvnpw/ai-security-analyzer/blob/main/examples/dir-vulnerabilities-screenshot-to-code-anthropicclaude-3.7-sonnetthinking.md)

| Vulnerability | Valid | Comment |
| --- | --- | --- |
| Prompt Injection Vulnerability in LLM Requests | ü§î | Well-described vulnerability. Interesting if it's real. Is it possible to do prompt injection by putting phrase on image? |
| Path Traversal in Evaluation Routes | ‚úÖ | Vulnerability is valid. Sonnet 3.7 also correctly identify that it's only possible to read `.html` files. |

## Vulnerability Workflow 1

Each run of tool can give a little bit different results. One time you can get 2 vulnerabilities, another time 3. It's random. So, I decided to run it multiple times and take the best result. I putted all into workflow. Let's see how it works.

### Graph

Workflow can be described with the following graph:

{{< figure src="https://github.com/user-attachments/assets/54b5fefe-57a6-40ae-9446-eb66d7c62de2"  class="image-center" >}}

Idea is to:
- use two models to find vulnerabilities: primary and secondary,
- use same prompt as for *normal* vulnerability analysis,
- add filter step to remove not relevant vulnerabilities,
- add merge step to get final list of vulnerabilities,

What is the benefit?
- having two different models, should give better results after all,
- what is more interesting, I can use those two models multiple times, getting two, three, four, etc. results for each model,
- all the merging and filtering is also done by LLM, so it's less manual work.

### Reliability

Graph looks promising, but there is catch. For each additional step involving LLM, can bring wrong results. In case of filtering, LLM can remove some valid vulnerabilities. In case of merging, LLM can wrongly merge lists or be lazy and return no results. 

## Testing the Workflow

For testing the workflow, I'll use the same project: `screenshot-to-code`. I will use different primary models and always same secondary model: `OpenAI o3-mini`.

| Primary Model | Secondary Model | ‚úÖ |  ü§î | ‚ùå | Comment |
| --- | --- | --- | --- | --- | --- |















