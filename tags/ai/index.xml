<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ai on xvnpw personal blog</title><link>https://xvnpw.github.io/tags/ai/</link><description>Recent content in Ai on xvnpw personal blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 18 May 2025 07:59:02 +0100</lastBuildDate><atom:link href="https://xvnpw.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Threat Modeling with LLMs: Two Years In - Hype, Hope, and a Look at Gemini 2.5 Pro</title><link>https://xvnpw.github.io/posts/threat-modeling-with-llms-two-years-in-hype-hope-and-a-look-at-gemini-2.5-pro/</link><pubDate>Sun, 18 May 2025 07:59:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/threat-modeling-with-llms-two-years-in-hype-hope-and-a-look-at-gemini-2.5-pro/</guid><description>&lt;p>It&amp;rsquo;s been nearly two years since I began exploring how AI can enhance threat modeling processes. In this post, I&amp;rsquo;ll share my latest findings with Gemini 2.5 Pro Preview, one of the newest advanced models, and reflect on how AI systems have evolved during this period. More importantly, I&amp;rsquo;ll examine whether they&amp;rsquo;re fulfilling their initial promise for security threat modeling applications.&lt;/p>
&lt;figure class="image-center">&lt;img src="https://xvnpw.github.io/threat-modeling-with-llms-two-years-in-hype-hope-and-a-look-at-gemini-2.5-pro.png" width="400">
&lt;/figure>

&lt;h2 id="the-experiment-testing-gemini-25-pro">The Experiment: Testing Gemini 2.5 Pro&lt;/h2>
&lt;p>As with my previous research, the goal is to assess how effectively LLMs can assist in threat modeling. For this, I used a deliberately underspecified architecture for a fictional project: &amp;ldquo;AI Nutrition Pro&amp;rdquo;.&lt;/p></description></item><item><title>Can AI Actually Find Real Security Bugs? Testing the New Wave of AI Models</title><link>https://xvnpw.github.io/posts/can-ai-actually-find-real-security-bugs-testing-the-new-wave-of-ai-models/</link><pubDate>Fri, 14 Feb 2025 16:00:00 +0100</pubDate><guid>https://xvnpw.github.io/posts/can-ai-actually-find-real-security-bugs-testing-the-new-wave-of-ai-models/</guid><description>&lt;p>Since the release of GPT-3.5, I&amp;rsquo;ve been experimenting with using Large Language Models (LLMs) to find vulnerabilities in source code. Initially, the results were underwhelming. LLMs frequently hallucinated or misidentified issues. However, the advent of &amp;ldquo;reasoning models&amp;rdquo; sparked my curiosity. Could these newer models, designed for more complex reasoning tasks, succeed where their predecessors struggled? This post documents my experiment to find out.&lt;/p>
&lt;figure class="image-center">&lt;img src="https://github.com/user-attachments/assets/746d9b98-e685-481a-8b8b-16f78f4fc32d" width="400">
&lt;/figure>

&lt;h2 id="the-challenge-llms-and-vulnerability-detection">The Challenge: LLMs and Vulnerability Detection&lt;/h2>
&lt;p>Can LLMs &lt;em>reliably&lt;/em> find vulnerabilities? It&amp;rsquo;s a complex question. Google&amp;rsquo;s &lt;a href="https://googleprojectzero.blogspot.com/2024/10/from-naptime-to-big-sleep.html">Project Zero&lt;/a> outlined a sophisticated approach involving an LLM agent, a debugger, and potentially other tools, all working together in a loop.&lt;/p></description></item><item><title>Forget Threats, Mitigations are All You REALLY Need</title><link>https://xvnpw.github.io/posts/forget-threats-mitigations-are-all-you-really-need/</link><pubDate>Sun, 02 Feb 2025 09:00:00 +0100</pubDate><guid>https://xvnpw.github.io/posts/forget-threats-mitigations-are-all-you-really-need/</guid><description>&lt;p>Ever feel like you&amp;rsquo;re speaking a different language when you talk to developers about security? You&amp;rsquo;re buzzing with threat models, attack vectors, and vulnerabilities, while they&amp;rsquo;re focused on features, deadlines, and, well, &lt;em>making things work&lt;/em>. I get it. I&amp;rsquo;ve been there. And recently, I had a bit of an &amp;ldquo;aha!&amp;rdquo; moment that completely shifted my perspective on security discussions.&lt;/p>
&lt;p>It happened during a review of security analysis with my colleagues at Form3. We were diving deep into the &lt;a href="https://github.com/form3tech-oss/terraform-provider-chronicle">terraform-provider-chronicle&lt;/a> project (you can see analysis &lt;a href="https://github.com/xvnpw/ai-security-analyzer/tree/main/examples/form3tech-oss/README.md">here&lt;/a>, using my &lt;a href="https://github.com/xvnpw/ai-security-analyzer">ai-security-analyzer&lt;/a> tool). As we talked through AI generated threats, it struck me: to make AI tools useful for developers, we need to focus on mitigations, not threats. Developers are less interested in abstract threats and far more engaged when you talk about &lt;strong>mitigations&lt;/strong> - concrete steps to &lt;em>fix&lt;/em> things.&lt;/p></description></item><item><title>Deep Analysis Mode in AI Security Analyzer</title><link>https://xvnpw.github.io/posts/ai-security-analyzer-deep-analysis-mode/</link><pubDate>Fri, 10 Jan 2025 20:00:00 +0100</pubDate><guid>https://xvnpw.github.io/posts/ai-security-analyzer-deep-analysis-mode/</guid><description>&lt;p>First off, &lt;strong>a big thank you&lt;/strong> ðŸ™‡ to everyone who provided such positive feedback on my previous post, &lt;a href="https://xvnpw.github.io/posts/scaling-threat-modeling-with-ai/">Scaling Threat Modeling with AI: Generating 1000 Threat Models Using Gemini 2.0 and AI Security Analyzer&lt;/a>. Your insights and suggestions have been incredibly valuable.&lt;/p>
&lt;p>Inspired by your comments, I&amp;rsquo;ve added a new feature to the AI Security Analyzer: &lt;strong>Deep Analysis Mode&lt;/strong>. In this post, I&amp;rsquo;ll walk you through how it works and showcase its capabilities using Google&amp;rsquo;s Gemini 2.0 Flash Thinking Experimental model to perform an in-depth threat modeling on the &lt;a href="https://github.com/pallets/flask">Flask&lt;/a> project. We&amp;rsquo;ll compare outputs between Normal Mode and Deep Analysis Mode to highlight the differences.&lt;/p></description></item><item><title>Scaling Threat Modeling with AI: Generating 1000 Threat Models Using Gemini 2.0 and AI Security Analyzer</title><link>https://xvnpw.github.io/posts/scaling-threat-modeling-with-ai/</link><pubDate>Wed, 01 Jan 2025 12:19:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/scaling-threat-modeling-with-ai/</guid><description>&lt;p>&amp;ldquo;Can AI help us scale security analysis?&amp;rdquo; This question led me down a fascinating path of experimenting with Google&amp;rsquo;s Gemini 2.0 to generate threat models at an unprecedented scale. In this post, I&amp;rsquo;ll share how I turned this ambitious idea into reality, complete with code samples, real outputs, and valuable lessons learned along the way.&lt;/p>
&lt;h2 id="the-challenge-automating-security-analysis-at-scale">The Challenge: Automating Security Analysis at Scale&lt;/h2>
&lt;p>Security documentation is crucial but often becomes a bottleneck in fast-moving development cycles. With the release of Google&amp;rsquo;s Gemini 2.0 Flash Thinking Experimental model, I saw an opportunity to tackle this challenge head-on, despite some notable limitations:&lt;/p></description></item><item><title>AI Security Analyzer - All-in-One Tool Preview</title><link>https://xvnpw.github.io/posts/ai-security-analyzer-all-in-one-tool-preview/</link><pubDate>Thu, 19 Dec 2024 08:59:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/ai-security-analyzer-all-in-one-tool-preview/</guid><description>&lt;p>&lt;a href="https://github.com/xvnpw/ai-security-analyzer">AI Security Analyzer&lt;/a> is my latest project - a powerful tool that leverages AI to automatically generate comprehensive security documentation for your projects. Whether you&amp;rsquo;re dealing with security design, threat modeling, attack surface analysis, or more, this tool aims to simplify and enhance your security documentation process.&lt;/p>
&lt;figure class="image-center">&lt;img src="https://github.com/user-attachments/assets/86ebd729-24ef-48cf-b704-3f42a8e34162" width="200">
&lt;/figure>

&lt;p>&lt;strong>Watch the demo:&lt;/strong>
&lt;figure class="image-center">&lt;img src="https://github.com/user-attachments/assets/a9de6ce7-9702-4fae-97a4-424d03a683eb">
&lt;/figure>
&lt;/p>
&lt;h2 id="threats-vs-vulnerabilities">Threats vs. Vulnerabilities&lt;/h2>
&lt;p>Before we dive in, I want to clarify an important distinction: &lt;strong>threats&lt;/strong> vs. &lt;strong>vulnerabilities&lt;/strong>. My tool focuses on identifying threats, which are &lt;strong>potential risks&lt;/strong> that could be exploited by an attacker. Vulnerabilities, on the other hand, are specific weaknesses in a system that can be exploited. If you&amp;rsquo;re looking for tools to identify vulnerabilities, I recommend checking out &lt;a href="https://googleprojectzero.blogspot.com/2024/10/from-naptime-to-big-sleep.html">Google Project Zero&lt;/a>.&lt;/p></description></item><item><title>Automating GitHub Workflows with Fabric Agent Action</title><link>https://xvnpw.github.io/posts/automating-github-workflows-with-fabric-agent-action/</link><pubDate>Fri, 22 Nov 2024 13:59:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/automating-github-workflows-with-fabric-agent-action/</guid><description>&lt;p>&lt;a href="https://github.com/xvnpw/fabric-agent-action">Fabric Agent Action&lt;/a> is a GitHub Action that bridges the gap between &lt;a href="https://github.com/danielmiessler/fabric">fabric&lt;/a> patterns and GitHub workflows. Instead of manually executing patterns or building custom integrations, you can automate complex tasks directly in your GitHub workflows using an agent-based approach.&lt;/p>
&lt;figure class="image-center">&lt;img src="https://github.com/user-attachments/assets/14110f11-1250-4792-8d1a-4da3fd85197e" width="200">
&lt;/figure>

&lt;p>The action introduces different types of agents, each designed for specific use cases, from simple pattern execution to complex reasoning about GitHub issues and pull requests. Let&amp;rsquo;s explore how these agents work and evaluate their effectiveness.&lt;/p></description></item></channel></rss>