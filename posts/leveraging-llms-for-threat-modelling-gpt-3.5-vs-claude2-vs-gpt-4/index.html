<!doctype html><html><head lang=en><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Leveraging LLMs for Threat Modeling - GPT-3.5 vs Claude 2 vs GPT-4 - xvnpw personal blog</title><link rel=icon type=image/png href=/favicon.png><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="I tested most important LLMs for ability to perform threat modeling. Let's check results and find out which performed best."><meta property="og:image" content><meta property="og:title" content="Leveraging LLMs for Threat Modeling - GPT-3.5 vs Claude 2 vs GPT-4"><meta property="og:description" content="I tested most important LLMs for ability to perform threat modeling. Let's check results and find out which performed best."><meta property="og:type" content="article"><meta property="og:url" content="https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5-vs-claude2-vs-gpt-4/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-09-03T08:59:02+01:00"><meta property="article:modified_time" content="2023-09-03T08:59:02+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Leveraging LLMs for Threat Modeling - GPT-3.5 vs Claude 2 vs GPT-4"><meta name=twitter:description content="I tested most important LLMs for ability to perform threat modeling. Let's check results and find out which performed best."><script src=https://xvnpw.github.io/js/feather.min.js></script>
<link href=https://xvnpw.github.io/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://xvnpw.github.io/css/main.ac08a4c9714baa859217f92f051deb58df2938ec352b506df655005dcaf98cc0.css><link id=darkModeStyle rel=stylesheet type=text/css href=https://xvnpw.github.io/css/dark.726cd11ca6eb7c4f7d48eb420354f814e5c1b94281aaf8fd0511c1319f7f78a4.css disabled><link rel=stylesheet type=text/css href=https://xvnpw.github.io/css/my.0b83e2a92adf566deab02a012390927b24d79d5dc5a21a839eb61419dc041acc.css></head><body><div class=content><header><div class=main><a href=https://xvnpw.github.io/>xvnpw personal blog</a></div><nav><a href=/>Home</a>
<a href=/posts>All posts</a>
<a href=/about>About</a>
<a href=/tags>Tags</a>
| <span id=dark-mode-toggle onclick=toggleTheme()></span>
<script src=https://xvnpw.github.io/js/themetoggle.js></script></nav></header><main><article><div class=title><h1 class=title>Leveraging LLMs for Threat Modeling - GPT-3.5 vs Claude 2 vs GPT-4</h1><div class=meta>Posted on Sep 3, 2023</div></div><section class=body><p>I tested the most important LLMs (GPT-3.5, Claude 2, and GPT-4) for ability to perform <strong>threat modeling</strong>. Let&rsquo;s check results and find out which performed best.</p><figure class=image-center><img src=https://github.com/xvnpw/xvnpw.github.io/assets/17719543/bf036c66-1d66-4468-8dcd-426d6e0f40f6></figure><p>This quote from <a href=https://twitter.com/emollick>Ethan Mollick</a>, who is professor at The Wharton School, resonates very much with my experience on LLMs. Answers from GPT look like someone tried to satisfy question in the easiest way possible üòè You can overcome that by asking for explanation or step-by-step thinking.</p><h2 id=experiment-structure-recap>Experiment structure recap</h2><p>If you want to learn more about structure of experiment check my <a href=https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5/>previous post</a>. Below is just short recap.</p><p>I prepared input data of markdown files that describe made up project called <strong>AI Nutrition-Pro</strong>:</p><ul><li><a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt3.5/blob/main/PROJECT.md>PROJECT.md</a> - high level project description</li><li><a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt3.5/blob/main/ARCHITECTURE.md>ARCHITECTURE.md</a> - architecture description</li><li><a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt3.5/blob/main/user-stories/0001_STORE_DIET_INTRODUCTIONS.md>0001_STORE_DIET_INTRODUCTIONS.md</a> - user story</li></ul><p>I defined 3 types of analysis to perform by LLMs:</p><ul><li>High level security design review (example: <a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt3.5/blob/main/PROJECT_SECURITY.md>GPT-3.5</a>, <a href=https://github.com/xvnpw/ai-nutrition-pro-design-claude2/blob/main/PROJECT_SECURITY.md>Claude 2</a>, <a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt4/blob/main/PROJECT_SECURITY.md>GPT-4</a>)</li><li>Threat Modeling (example: <a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt3.5/blob/main/ARCHITECTURE_SECURITY.md>GPT-3.5</a>, <a href=https://github.com/xvnpw/ai-nutrition-pro-design-claude2/blob/main/ARCHITECTURE_SECURITY.md>Claude 2</a>, <a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt4/blob/main/ARCHITECTURE_SECURITY.md>GPT-4</a>)</li><li>Security related acceptance criteria (example: <a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt3.5/blob/main/user-stories/0001_STORE_DIET_INTRODUCTIONS_SECURITY.md>GPT-3.5</a>, <a href=https://github.com/xvnpw/ai-nutrition-pro-design-claude2/blob/main/user-stories/0001_STORE_DIET_INTRODUCTIONS_SECURITY.md>Claude 2</a>, <a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt4/blob/main/user-stories/0001_STORE_DIET_INTRODUCTIONS_SECURITY.md>GPT-4</a>)</li></ul><h2 id=interesting-bits>Interesting bits</h2><h3 id=assumptions>Assumptions</h3><p>Threat modeling is art and science. One reason for that is <strong>assumptions</strong> that are not well specified. Those assumptions filter possible threats and drive whole process. In my research, I didn&rsquo;t set any assumptions in prompts to see how LLMs can handle that. I can tell you that LLM behaves the same as someone new to threat modeling, and starts giving many different injection attacks (starting with SQL Injections&mldr;).</p><p>You can overcome that by listing assumptions directly:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Instruction:
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>- I will provide you Assumptions that you need to consider in listing threats
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Assumptions:
</span></span><span style=display:flex><span>- SQL Injection threat is already mitigated using SAST tool
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h3 id=explanation>Explanation</h3><p>I learned that asking LLMs to explain why threat is applicable to architecture can give good results. It made list of threats less random.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Instruction:
</span></span><span style=display:flex><span>- Explanation whether or not this threat is already mitigated in architecture
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>There is also a big difference between GPT-3.5 and GPT-4 in understanding explanation.</p><p><strong>GPT-3.5</strong>:</p><ul><li><strong>Threat name:</strong> Attacker spoofs client identity and gains unauthorized access to API Gateway</li><li><strong>Explanation:</strong> This threat is applicable as API Gateway is the entry point for client requests and authentication is required to ensure the client&rsquo;s identity.</li><li><strong>Mitigations:</strong> Implement strong authentication mechanisms such as API keys, OAuth, or JWT tokens to verify the client&rsquo;s identity.</li></ul><p><strong>GPT-4</strong>:</p><ul><li><strong>Threat name:</strong> Attacker bypasses API Gateway and directly accesses API Application</li><li><strong>Explanation:</strong> This threat is applicable if the API Application is not properly secured and can be accessed without going through the API Gateway.</li><li><strong>Mitigations:</strong> Ensure that the API Application is only accessible through the API Gateway and cannot be directly accessed.</li></ul><p>It could be taken into higher level with step by step thinking. But didn&rsquo;t test that yet:</p><blockquote><p>When I give you something to do, you will convert that to a step by step plan and tell me what the step by step plan is.</p></blockquote><h3 id=components>Components</h3><p>What is component? This <em>word</em> is so overloaded that can mean almost anything. And LLMs have big problem in understanding it. Especially GPT-3.5 and Claude 2.</p><p>Why component? A technique that I choose for threat modeling is &ldquo;STRIDE per component&rdquo; (or you can call it STRIDE per element).</p><p>After few tries I ended up with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Instruction:
</span></span><span style=display:flex><span>- List data flows that starts from internal or ends in internal architecture container that are important for security of system
</span></span><span style=display:flex><span>- Data flow should contain two items: A -&gt; B (A and B are architecture containers)
</span></span><span style=display:flex><span>- Architecture container is for example: service, microservice, database, queue, person, gateway, lambda, application
</span></span><span style=display:flex><span>- Architecture containers are included in C4 model in Container diagram
</span></span></code></pre></div><p>It&rsquo;s not perfect, because it expects architecture to be C4 model. This limits flexibility of prompt.</p><p>BTW. GPT-4 can understand what component is üòä</p><h3 id=json>JSON</h3><p>I started experiment asking LLM to format output as markdown. I needed to change that when added instruction to create explanation (I not always show it). I decided to switch to json and make markdown myself. Both GPTs are good at creating proper json, but <strong>Claude 2 is not so good</strong>. From time to time it returned malformed document. Solution was to use <a href=https://python.langchain.com/docs/modules/model_io/output_parsers/output_fixing_parser>OutputFixingParser</a> from <code>langchain</code>. If it detects malformed json it ask LLM to fix it üòè</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>fixing_parser = OutputFixingParser.from_llm(parser=parser, llm=llm)
</span></span><span style=display:flex><span>gen_threats = fixing_parser.parse(ret)
</span></span></code></pre></div><h2 id=gpt-35-vs-claude-2-vs-gpt-4>GPT-3.5 vs Claude 2 vs GPT-4</h2><ul><li>GPT-3.5 and Claude 2 gave comparable results. They are good and can be used if you have development team that is not so much experienced in security. Results will give you reasonable amount of most important things to consider.</li><li>GPT-4 gave best results. It&rsquo;s less fragile to changes in prompt. Using assumptions, one can build really good automation leveraging AI to perform threat modeling.</li></ul><p>üî• Check results yourself: <a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt3.5/blob/main/ARCHITECTURE_SECURITY.md>GPT-3.5</a>, <a href=https://github.com/xvnpw/ai-nutrition-pro-design-claude2/blob/main/ARCHITECTURE_SECURITY.md>Claude 2</a>, <a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt4/blob/main/ARCHITECTURE_SECURITY.md>GPT-4</a> and let me know what you think about experiment!</p><h2 id=summary>Summary</h2><p>I have tested 3 the most important LLMs for capabilities in doing threat modeling. All of them gave good results when asked about most important threats. <strong>GPT-4</strong> is most promising in tuning for particular real-life needs. Because it can understand assumptions and connect them to threats. It will not create threat about SQL Injection if informed that this threat is already mitigated elsewhere.</p><p><a href=https://github.com/xvnpw/ai-threat-modeling-action>Code</a> used in this experiment is published on github.</p><hr><p>Thanks for reading! You can contact me and/or follow on <a href=https://twitter.com/xvnpw>X/Twitter</a>.</p></section><div class=post-tags><nav class="nav tags"><ul class=tags><li><a href=/tags/security>security</a></li><li><a href=/tags/threat-modeling>threat-modeling</a></li><li><a href=/tags/langchain>langchain</a></li><li><a href=/tags/llm>llm</a></li><li><a href=/tags/gpt>gpt</a></li><li><a href=/tags/claude>claude</a></li></ul></nav></div></article></main><footer><div style=display:flex><a class=soc href=https://github.com/xvnpw rel=me title=GitHub><i data-feather=github></i></a>
<a class=border></a><a class=soc href=https://twitter.com/xvnpw rel=me title=Twitter><i data-feather=twitter></i></a>
<a class=border></a></div><div class=footer-info>2023 <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a></div></footer><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-YHETMXZXMZ","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script>
<script>feather.replace()</script></div></body></html>