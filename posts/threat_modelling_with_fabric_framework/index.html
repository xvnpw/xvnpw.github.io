<!doctype html><html><head lang=en><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Threat Modelling with Fabric Framework - xvnpw personal blog</title><link rel=icon type=image/png href=/favicon.png><meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="The Fabric framework enhances AI-powered threat modeling with a new pattern, offering detailed, actionable security insights."><meta property="og:image" content><meta property="og:url" content="https://xvnpw.github.io/posts/threat_modelling_with_fabric_framework/"><meta property="og:site_name" content="xvnpw personal blog"><meta property="og:title" content="Threat Modelling with Fabric Framework"><meta property="og:description" content="The Fabric framework enhances AI-powered threat modeling with a new pattern, offering detailed, actionable security insights."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-03T16:59:02+01:00"><meta property="article:modified_time" content="2024-06-03T16:59:02+01:00"><meta property="article:tag" content="Security"><meta property="article:tag" content="Threat-Modeling"><meta property="article:tag" content="Fabric"><meta property="article:tag" content="Llm"><meta property="article:tag" content="Gpt"><meta property="article:tag" content="Claude"><meta name=twitter:card content="summary"><meta name=twitter:title content="Threat Modelling with Fabric Framework"><meta name=twitter:description content="The Fabric framework enhances AI-powered threat modeling with a new pattern, offering detailed, actionable security insights."><script src=https://xvnpw.github.io/js/feather.min.js></script><link href=https://xvnpw.github.io/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://xvnpw.github.io/css/main.5cebd7d4fb2b97856af8d32a6def16164fcf7d844e98e236fcb3559655020373.css><link id=darkModeStyle rel=stylesheet type=text/css href=https://xvnpw.github.io/css/dark.d22e2a2879d933a4b781535fc4c4c716e9f9d35ea4986dd0cbabda82effc4bdd.css disabled><link rel=stylesheet type=text/css href=https://xvnpw.github.io/css/my.0b83e2a92adf566deab02a012390927b24d79d5dc5a21a839eb61419dc041acc.css></head><body><div class=content><header><div class=main><a href=https://xvnpw.github.io/>xvnpw personal blog</a></div><nav><a href=/>Home</a>
<a href=/posts>All posts</a>
<a href=/about>About</a>
<a href=/tags>Tags</a>
| <span id=dark-mode-toggle onclick=toggleTheme()></span>
<script src=https://xvnpw.github.io/js/themetoggle.js></script></nav></header><main><article><div class=title><h1 class=title>Threat Modelling with Fabric Framework</h1><div class=meta>Posted on Jun 3, 2024</div></div><section class=body><p><a href=https://github.com/danielmiessler/fabric>Fabric</a> is a framework that puts AI at your fingertips. Instead of diving into chat interfaces (e.g., ChatGPT) or writing custom programs that consume APIs, you can create prompts as markdown text and receive output in markdown. Fabric also maintains a database of prompts called <a href=https://github.com/danielmiessler/fabric/tree/main/patterns>patterns</a>.</p><figure class=image-center><img src=https://github.com/xvnpw/xvnpw.github.io/assets/17719543/4fe91d36-3736-4cbf-9835-edfa3943116e width=300></figure><p>With the new pattern <a href=https://github.com/danielmiessler/fabric/blob/main/patterns/create_stride_threat_model/system.md>create_stride_threat_model</a>, you can easily create threat models. Let&rsquo;s dive deeper into how to use this new pattern and evaluate the quality of the results.</p><h2 id=new-pattern-in-action>New Pattern in Action</h2><p>From my <a href=https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-claude-3-vs-gpt-4/>previous post</a>, you can learn about my experiment on using Large Language Models for threat modeling. In this article, we will use the architecture document of a fictional project called &ldquo;AI Nutrition-Pro&rdquo; as input:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span># Get Fabric installed - https://github.com/danielmiessler/fabric
</span></span><span style=display:flex><span>$ wget https://raw.githubusercontent.com/xvnpw/fabric-stride-threat-model/main/INPUT.md
</span></span><span style=display:flex><span>$ cat INPUT.md | fabric --pattern create_stride_threat_model -m claude-3-opus-20240229
</span></span></code></pre></div><p>In this case, I use the <code>claude-3-opus-20240229</code> model. At the time of writing, Claude 3 Opus represents the best model for threat modeling in my opinion.</p><h2 id=beginning-of-the-story---creating-a-new-pattern>Beginning of the Story - Creating a New Pattern</h2><p>Before we jump into the results, I first needed to create a new pattern in Fabric. To do that, I used three baseline threat models created for the same input file. This allowed me to compare results with already existing and tested solutions:</p><table><thead><tr><th>Threat Model</th><th>Link</th><th>LLM Model</th></tr></thead><tbody><tr><td>Baseline threat model created by Fabric using the <a href=https://github.com/danielmiessler/fabric/blob/main/patterns/create_threat_scenarios/system.md>create_threat_scenarios</a> pattern</td><td><a href=https://github.com/xvnpw/fabric-stride-threat-model/blob/main/baseline_threat_scenarios.md>baseline_threat_scenarios.md</a></td><td>Claude 3 Opus</td></tr><tr><td>Baseline threat model created by <a href=https://github.com/mrwadams/stride-gpt>STRIDE GPT</a></td><td><a href=https://github.com/xvnpw/fabric-stride-threat-model/blob/main/baseline_stride_gpt.md>baseline_stride_gpt.md</a></td><td>GPT-4o</td></tr><tr><td>Baseline threat model from my <a href=https://github.com/xvnpw/ai-threat-modeling-action>ai-threat-modeling-action</a></td><td><a href=https://github.com/xvnpw/fabric-stride-threat-model/blob/main/baseline_threat_modeling_action.md>baseline_threat_modeling_action.md</a></td><td>Claude 3 Opus</td></tr></tbody></table><p>These three solutions are unique in their approach:</p><ul><li><code>create_threat_scenarios</code> is the most generic prompt, and I was very surprised to see concrete threats and mitigations there.</li><li><code>STRIDE GPT</code> gives a different perspective. Before you can run threat generation, you need to answer a few questions. I picked the application type as <strong>cloud</strong>, sensitivity of data as <strong>confidential</strong>, said <strong>yes</strong> for internet-facing, and picked <strong>Basic</strong> auth. The results are very interesting. Threats are not grouped for specific data flows or components; instead, they are listed as scenarios, similar to the <code>create_threat_scenarios</code> approach.</li><li>Finally, my previous work - <code>ai-threat-modeling-action</code>. I chose a different approach by splitting the design by data flows and components. The AI has more work to do (and it takes several round trips to model). The advantage is having a threat model using the STRIDE per element methodology as I would imagine it to be. The disadvantage is that AI sometimes misunderstands components, elements, or data flows. You cannot find any reference to AWS or cloud in this threat model. In both previous threat models, you can find some threats for AWS ECS or RDS.</li></ul><p>Considering that the already existing pattern <code>create_threat_scenarios</code> is not based on STRIDE and lists threats without distinction on components and data flows, I decided to use the STRIDE per element approach. This way, Fabric will have two patterns for threat modeling but different.</p><h2 id=interrogating-ai>Interrogating AI</h2><p>Interrogation is the best way to describe my prompt. You may already notice it in <code>ai-threat-modeling-action</code>, where there are columns like <em>Explanation</em> or <em>How threat is already mitigated in architecture</em>. Why do that? There are a few points here:</p><ul><li>LLMs generate the next token based on previously generated ones - by having more details, I hope for better output.</li><li>LLMs consider the semantics of things - by grouping and relating ideas in multi-dimensional space - something like clustering - getting back to the first point, more related tokens should lead to better output.</li><li>It gives a view on how LLM &ldquo;understands&rdquo; input - it provides an opportunity to get back and improve design documents.</li></ul><h3 id=output-format>Output Format</h3><p>I organized the output as markdown with the following sections:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span># ASSETS
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Section to determine what data or assets need protection
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># TRUST BOUNDARIES
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Section to identify and list all trust boundaries. 
</span></span><span style=display:flex><span>Trust boundaries represent the border between trusted and untrusted elements.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># DATA FLOWS
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Section to identify and list all data flows between components. 
</span></span><span style=display:flex><span>Data flow is interaction between two components. Mark data flows crossing trust boundaries.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># THREAT MODEL
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Section with table of STRIDE per element threats. 
</span></span><span style=display:flex><span>Prioritize threats by likelihood and potential impact.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Table with columns:
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> THREAT ID 
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> COMPONENT NAME
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> THREAT NAME
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> STRIDE CATEGORY
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> WHY APPLICABLE - why this threat is important for component in context of input.
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> HOW MITIGATED - how threat is already mitigated in architecture - explain if this 
</span></span><span style=display:flex><span>threat is already mitigated in design (based on input) or not. Give reference to input.
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> MITIGATION - provide mitigation that can be applied for this threat. It should be 
</span></span><span style=display:flex><span>detailed and related to input.
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> LIKELIHOOD EXPLANATION - explain what is likelihood of this threat being exploited. 
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> IMPACT EXPLANATION - explain impact of this threat being exploited. 
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> RISK SEVERITY - risk severity of threat being exploited. Based it on LIKELIHOOD and IMPACT. 
</span></span><span style=display:flex><span>Give value, e.g.: low, medium, high, critical.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># QUESTIONS &amp; ASSUMPTIONS
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Section to list questions that you have and the default assumptions regarding THREAT MODEL.
</span></span></code></pre></div><h2 id=comparison-of-results-to-baselines>Comparison of Results to Baselines</h2><p>Let&rsquo;s compare in detail how the new pattern differs from baselines:</p><table><thead><tr><th>Difference</th><th>To</th><th>Comment</th></tr></thead><tbody><tr><td>Lower number of threats</td><td>All baselines</td><td>This is an obvious difference. All baselines have more threats. <strong>Why?</strong> The prompt is very much focused on &ldquo;likelihood,&rdquo; &ldquo;impact,&rdquo; &ldquo;what&rsquo;s worth defending,&rdquo; no &ldquo;fantastical concerns&rdquo; (credits to Daniel Miessler). I can request more threats by explicitly forcing it to &ldquo;generate at least 10 threats&rdquo; or &ldquo;create at least 10 rows in the threats table&rdquo;, but those threats are not high priority (in my opinion) anymore. I can&rsquo;t help but refer this to Isaac Asimov&rsquo;s Robot Series of books. Robots there behaved similarly, and skilled prompting was something like art.</td></tr><tr><td>No cloud threats</td><td><code>create_threat_scenarios</code> and STRIDE GPT</td><td>I mentioned this before. I think STRIDE per element is a bit old methodology and not mentioned in publications (=training data) in case of cloud.</td></tr><tr><td>Important and actionable threats for startups</td><td>All baselines</td><td>I got only <strong>5 threats</strong> generated by the new pattern, but those are very important and you need to act if you were a startup. Especially in the case of STRIDE GPT and <code>ai-threat-modeling-action</code>, you can get a feeling of mature enterprise and not startup.</td></tr><tr><td>Interrogation</td><td><code>create_threat_scenarios</code> and STRIDE GPT</td><td>I extended descriptions on why, how, and what from those in <code>ai-threat-modeling-action</code>. In my opinion, it&rsquo;s important to get more output. This can help us define input and understand why we got certain threats.</td></tr></tbody></table><h2 id=threat-models>Threat Models</h2><p>It&rsquo;s time to check the results of my new pattern. To make things more interesting, I ran it for three different LLM models:</p><ul><li><a href=https://github.com/xvnpw/fabric-stride-threat-model/blob/main/threat_model_claude_3_opus.md>Threat model</a> created using Claude 3 Opus</li><li><a href=https://github.com/xvnpw/fabric-stride-threat-model/blob/main/threat_model_gpt_4o.md>Threat model</a> created using GPT-4o</li><li><a href=https://github.com/xvnpw/fabric-stride-threat-model/blob/main/threat_model_gemini_1.5_pro.md>Threat model</a> created using Gemini-1.5 Pro Latest</li></ul><p>In my opinion, Claude 3 Opus is the best and GPT-4o second. There is a noticeable gap between these two and Gemini 1.5 Pro (which is worst).</p><h3 id=assets-section>Assets Section</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span># ASSETS
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The following data and assets need protection in the AI Nutrition-Pro system:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>1.</span> Dietitians&#39; content samples stored in the API database
</span></span><span style=display:flex><span><span style=color:#66d9ef>2.</span> Request and response data related to LLM interactions stored in the API database
</span></span><span style=display:flex><span><span style=color:#66d9ef>3.</span> Control plane data (tenant information, billing data, configuration) stored in the Control Plane Database
</span></span><span style=display:flex><span><span style=color:#66d9ef>4.</span> API keys used for authentication of Meal Planner applications
</span></span><span style=display:flex><span><span style=color:#66d9ef>5.</span> Access credentials for the ChatGPT-3.5 LLM service
</span></span></code></pre></div><p>It&rsquo;s correct.</p><h3 id=trust-boundaries-section>Trust Boundaries Section</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span># TRUST BOUNDARIES
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The following trust boundaries are identified in the AI Nutrition-Pro system:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>1.</span> Between the Meal Planner application and the API Gateway
</span></span><span style=display:flex><span><span style=color:#66d9ef>2.</span> Between the API Gateway and the API Application
</span></span><span style=display:flex><span><span style=color:#66d9ef>3.</span> Between the API Application and the ChatGPT-3.5 LLM service
</span></span><span style=display:flex><span><span style=color:#66d9ef>4.</span> Between the Web Control Plane and the Control Plane Database
</span></span><span style=display:flex><span><span style=color:#66d9ef>5.</span> Between the API Application and the API database
</span></span></code></pre></div><p>Trust boundaries are subjective, but in this case, I would argue that AI did it <strong>wrong</strong>. It&rsquo;s something that might need a better prompt or more explicit input document.</p><h3 id=data-flows-section>Data Flows Section</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span># DATA FLOWS
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>The following data flows are identified in the AI Nutrition-Pro system:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>1.</span> Meal Planner application uploads dietitians&#39; content samples to AI Nutrition-Pro via API Gateway (crosses trust boundary)
</span></span><span style=display:flex><span><span style=color:#66d9ef>2.</span> Meal Planner application fetches AI-generated results from AI Nutrition-Pro via API Gateway (crosses trust boundary)
</span></span><span style=display:flex><span><span style=color:#66d9ef>3.</span> API Gateway forwards requests to API Application for processing (crosses trust boundary)
</span></span><span style=display:flex><span><span style=color:#66d9ef>4.</span> API Application sends requests to ChatGPT-3.5 LLM service for content generation (crosses trust boundary)
</span></span><span style=display:flex><span><span style=color:#66d9ef>5.</span> Web Control Plane reads/writes data to Control Plane Database
</span></span><span style=display:flex><span><span style=color:#66d9ef>6.</span> API Application reads/writes data to API database
</span></span></code></pre></div><p>These data flows are correct; however, data flows 5 and 6 also cross trust boundaries per the previous section. Something is not correct here.</p><h3 id=threat-model-section>Threat Model Section</h3><h4 id=spoofing-of-api-gateway>Spoofing of API Gateway</h4><p>I will show you only a single threat: <strong>spoofing</strong> of API Gateway:</p><table><thead><tr><th>Column</th><th>Value</th><th>Comment</th></tr></thead><tbody><tr><td>THREAT ID</td><td>0001</td><td>✅</td></tr><tr><td>COMPONENT NAME</td><td>API Gateway</td><td>✅</td></tr><tr><td>THREAT NAME</td><td>Unauthorized access to API endpoints</td><td>✅</td></tr><tr><td>STRIDE CATEGORY</td><td>Spoofing</td><td>✅</td></tr><tr><td>WHY APPLICABLE</td><td>Attackers may try to access API endpoints without proper authentication to gain unauthorized access to data or functionality.</td><td>Somehow good but also generic</td></tr><tr><td>HOW MITIGATED</td><td>Authentication with API keys for each Meal Planner application is implemented.</td><td>✅ - this is exactly mentioned in input document</td></tr><tr><td>MITIGATION</td><td>Ensure strong API key generation, secure storage, and regular rotation. Implement rate limiting and input validation at the API Gateway level.</td><td>✅</td></tr><tr><td>LIKELIHOOD EXPLANATION</td><td>Low, as authentication and rate limiting are in place, making it difficult for attackers to gain unauthorized access.</td><td>✅</td></tr><tr><td>IMPACT EXPLANATION</td><td>High, as unauthorized access could lead to data breaches or misuse of the system.</td><td>✅</td></tr><tr><td>RISK SEVERITY</td><td>Medium</td><td>✅ - likelihood x impact</td></tr></tbody></table><h4 id=comparison-to-baselines>Comparison to Baselines</h4><p>Let&rsquo;s see how spoofing of API Gateway is described in baseline threat models:</p><table><thead><tr><th>Baseline Name</th><th>Threat Name</th><th>Mitigations</th><th>Comment</th></tr></thead><tbody><tr><td><code>create_threat_scenarios</code> from Fabric</td><td>Unauthorized access to API Gateway via stolen API keys.</td><td>None</td><td>We get a focus on stolen API key, which is most likely way to obtain an API key by an attacker. Sadly no mitigation.</td></tr><tr><td>STRIDE GPT</td><td>An attacker uses a stolen API key to impersonate a Meal Planner application.</td><td>Implement API key rotation and revocation policies. Use mutual TLS for authentication and enforce strict validation of certificates. Implement anomaly detection to identify unusual API usage patterns.</td><td>Both threat and mitigations are very good; once again theft of API keys is mentioned.</td></tr><tr><td>ai-threat-modeling-action</td><td>Attacker bypasses weak authentication and gains unauthorized access to API Gateway</td><td>Ensure strong authentication mechanisms are in place, such as using secure and properly implemented API keys or OAuth tokens. Regularly rotate and revoke API keys. Implement rate limiting and monitoring to detect and prevent brute-force attempts.</td><td>Didn&rsquo;t mention stolen API keys as previous ones; similar overall.</td></tr></tbody></table><p>It&rsquo;s very good that <code>create_threat_scenarios</code> and STRIDE GPT give more meaningful attack scenarios with stolen API keys; it&rsquo;s realistic and can be used to convince stakeholders for investing into mitigations.</p><h3 id=questions--assumptions-section>Questions & Assumptions Section</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span><span style=color:#66d9ef>1.</span> Q: Are there any additional security measures in place for the API database, such as encryption at rest or strong access controls?
</span></span><span style=display:flex><span>   A: The design document does not explicitly mention encryption at rest or specific access controls for the API database. It is assumed that standard security practices are followed, but additional measures may be necessary to protect sensitive data.
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>This section can be a source of inspiration to update input documents in order to make them more clear and explicit.</p><h2 id=key-learnings>Key Learnings</h2><h3 id=context-matters>Context Matters</h3><p>There is something missing in all solutions to threat modeling that we checked today: <strong>context</strong>. In real life, we don&rsquo;t create a threat model for its own sake but with a purpose in mind. Context can describe what kind of company will implement this design; it could be a startup company just starting out with AI Nutrition-Pro as their only project or a mature company where AI Nutrition-Pro will be one of many features in their platform. Risk appetite and existing platform or company security posture will matter here; also it will be different who will be consuming the threat model; developers or security team?</p><p>It&rsquo;s definitely worth checking how LLM can handle this additional context; but this is for another experiment.</p><h3 id=fabric-makes-integration-easy>Fabric Makes Integration Easy</h3><p>If you are console-oriented, Fabric will be perfect for you.</p><h3 id=co-intelligence>Co-intelligence</h3><p>Ethan Mollick coined this term <em>co-intelligence</em> as a way to describe how LLMs help us in thinking processes. I cannot predict the future but have a feeling that LLMs will become tools that help us think and solve problems better; there is no security engineer to replace in companies that don&rsquo;t hire any in the first place; there are only developers who will do their job better using AI.</p><p><a href=https://github.com/xvnpw/fabric-stride-threat-model>Code</a> used in this experiment is published on GitHub.</p><hr><p>Thanks for reading! You can contact me and/or follow me on <a href=https://x.com/xvnpw>X</a>.</p></section><div class=post-tags><nav class="nav tags"><ul class=tags><li><a href=/tags/security>security</a></li><li><a href=/tags/threat-modeling>threat-modeling</a></li><li><a href=/tags/fabric>fabric</a></li><li><a href=/tags/llm>llm</a></li><li><a href=/tags/gpt>gpt</a></li><li><a href=/tags/claude>claude</a></li></ul></nav></div></article></main><footer><div style=display:flex><a class=soc href=https://github.com/xvnpw rel=me title=GitHub><i data-feather=github></i></a>
<a class=border></a><a class=soc href=https://twitter.com/xvnpw rel=me title=Twitter><i data-feather=twitter></i></a>
<a class=border></a><a class=soc href=https://www.linkedin.com/in/marcin-niemiec-304349104/ rel=me title=Linkedin><i data-feather=linkedin></i></a>
<a class=border></a></div><div class=footer-info>2024 <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a></div></footer><script async src="https://www.googletagmanager.com/gtag/js?id=G-YHETMXZXMZ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-YHETMXZXMZ")}</script><script>feather.replace()</script></div></body></html>