<!doctype html><html><head lang=en><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>xvnpw personal blog | Home </title><link rel=icon type=image/png href=/favicon.png><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content><meta property="og:image" content><link rel=alternate type=application/rss+xml href=https://xvnpw.github.io/index.xml title="xvnpw personal blog"><meta property="og:url" content="https://xvnpw.github.io/"><meta property="og:site_name" content="xvnpw personal blog"><meta property="og:title" content="xvnpw personal blog"><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="xvnpw personal blog"><script src=https://xvnpw.github.io/js/feather.min.js></script><link href=https://xvnpw.github.io/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://xvnpw.github.io/css/main.5cebd7d4fb2b97856af8d32a6def16164fcf7d844e98e236fcb3559655020373.css><link id=darkModeStyle rel=stylesheet type=text/css href=https://xvnpw.github.io/css/dark.d22e2a2879d933a4b781535fc4c4c716e9f9d35ea4986dd0cbabda82effc4bdd.css media="(prefers-color-scheme: dark)"><link rel=stylesheet type=text/css href=https://xvnpw.github.io/css/my.3c46484c36967b544a264fefb69e90ef0756aacf691c82f058e0f58769499ad2.css></head><body><div class=content><header><div class=main><a href=https://xvnpw.github.io/>xvnpw personal blog</a></div><nav><a href=/>Home</a>
<a href=/posts>All posts</a>
<a href=/about>About</a>
<a href=/tags>Tags</a></nav></header><main class=list><div class=site-description><p>hacking, bug bounty, appsec</p></div><section class=list-item><h1 class=title><a href=/posts/ai-security-analyzer-deep-analysis-mode/>Deep Analysis Mode in AI Security Analyzer</a></h1><time>Jan 10, 2025</time><br><div class=description>Discover how the new Deep Analysis Mode in AI Security Analyzer provides in-depth security insights, with practical examples using Google's Gemini 2.0 Flash Thinking Experimental model.</div><a class=readmore href=/posts/ai-security-analyzer-deep-analysis-mode/>Read more ⟶</a></section><section class=list-item><h1 class=title><a href=/posts/scaling-threat-modeling-with-ai/>Scaling Threat Modeling with AI: Generating 1000 Threat Models Using Gemini 2.0 and AI Security Analyzer</a></h1><time>Jan 1, 2025</time><br><div class=description>An in-depth look at how I leveraged Gemini 2.0 to create a massive security documentation repository, complete with practical examples and lessons learned.</div><a class=readmore href=/posts/scaling-threat-modeling-with-ai/>Read more ⟶</a></section><section class=list-item><h1 class=title><a href=/posts/ai-security-analyzer-all-in-one-tool-preview/>AI Security Analyzer - All-in-One Tool Preview</a></h1><time>Dec 19, 2024</time><br><div class=description>Preview of the AI Security Analyzer - a new tool that leverages AI to automatically generate comprehensive security design documentation for your projects.</div><a class=readmore href=/posts/ai-security-analyzer-all-in-one-tool-preview/>Read more ⟶</a></section><section class=list-item><h1 class=title><a href=/posts/automating-github-workflows-with-fabric-agent-action/>Automating GitHub Workflows with Fabric Agent Action</a></h1><time>Nov 22, 2024</time><br><div class=description>Introducing the Fabric Agent Action - a GitHub Action that automates complex workflows using AI-powered agents and Fabric Patterns.</div><a class=readmore href=/posts/automating-github-workflows-with-fabric-agent-action/>Read more ⟶</a></section><section class=list-item><h1 class=title><a href=/posts/fabric_design_documents/>Create design documents with Fabric</a></h1><time>Oct 30, 2024</time><br><div class=description>How I use Fabric patterns to create, review and refine design documents.</div><a class=readmore href=/posts/fabric_design_documents/>Read more ⟶</a></section><section class=list-item><h1 class=title><a href=/posts/threat_modelling_with_fabric_framework/>Threat Modelling with Fabric Framework</a></h1><time>Jun 3, 2024</time><br><div class=description>The Fabric framework enhances AI-powered threat modeling with a new pattern, offering detailed, actionable security insights.</div><a class=readmore href=/posts/threat_modelling_with_fabric_framework/>Read more ⟶</a></section><section class=list-item><h1 class=title><a href=/posts/leveraging-llms-for-threat-modelling-claude-3-vs-gpt-4/>Leveraging LLMs for Threat Modeling - Claude 3 Opus vs GPT-4</a></h1><time>Mar 20, 2024</time><br><div class=description>With new version of Claude model, I would like to compare it to GPT-4 in threat modeling.</div><a class=readmore href=/posts/leveraging-llms-for-threat-modelling-claude-3-vs-gpt-4/>Read more ⟶</a></section><section class=list-item><h1 class=title><a href=/posts/review_your_architecture_using_llms/>Reviewing Your Architecture Using LLMs</a></h1><time>Oct 25, 2023</time><br><div class=description>The quality of input data is crucial for LLMs to perform effectively. Learn how you can use these LLMs to improve your architectural descriptions. Explore the new feature in my ai-threat-modeling-action GitHub action.</div><a class=readmore href=/posts/review_your_architecture_using_llms/>Read more ⟶</a></section><section class=list-item><h1 class=title><a href=/posts/leveraging-llms-for-threat-modelling-gpt-3.5-vs-claude2-vs-gpt-4/>Leveraging LLMs for Threat Modeling - GPT-3.5 vs Claude 2 vs GPT-4</a></h1><time>Sep 3, 2023</time><br><div class=description>We put the leading AI models to the test in threat modeling. Let's dive into the results and see which one comes out on top.</div><a class=readmore href=/posts/leveraging-llms-for-threat-modelling-gpt-3.5-vs-claude2-vs-gpt-4/>Read more ⟶</a></section><section class=list-item><h1 class=title><a href=/posts/leveraging-llms-for-threat-modelling-gpt-3.5/>Leveraging LLMs for Threat Modeling - GPT-3.5</a></h1><time>Aug 17, 2023</time><br><div class=description>In this article, I delve into the AI Nutrition-Pro experiment, a research project exploring the potential of LLMs in enhancing security practices during the design phase of DevSecOps: threat modeling and security review.</div><a class=readmore href=/posts/leveraging-llms-for-threat-modelling-gpt-3.5/>Read more ⟶</a></section><ul class=pagination><span class="page-item page-prev"></span><span class="page-item page-next"><a href=/page/2/ class=page-link aria-label=Next><span aria-hidden=true>Next →</span></a></span></ul></main><footer><div style=display:flex><a class=soc href=https://github.com/xvnpw rel=me title=GitHub><i data-feather=github></i></a>
<a class=border></a><a class=soc href=https://twitter.com/xvnpw rel=me title=Twitter><i data-feather=twitter></i></a>
<a class=border></a><a class=soc href=https://www.linkedin.com/in/marcin-niemiec-304349104/ rel=me title=Linkedin><i data-feather=linkedin></i></a>
<a class=border></a></div><div class=footer-info>2025 <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a></div></footer><script async src="https://www.googletagmanager.com/gtag/js?id=G-YHETMXZXMZ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-YHETMXZXMZ")}</script><script>feather.replace()</script></div></body></html>