---
title: "Threat Modeling with LLMs: Two Years In - Hype, Hope, and a Look at Gemini 2.5 Pro"
date: 2025-05-18T08:59:02+01:00
draft: true
tags: [ai, llm, threat modeling, cybersecurity, chatgpt, gemini]
description: "After two years of exploring AI for threat modeling, this post reviews the progress, tests Gemini 2.5 Pro, and reflects on the evolving potential and limitations of LLMs in cybersecurity."
---

It's been nearly two years since I began exploring how AI can enhance threat modeling processes. In this post, I'll share my latest findings with Gemini 2.5 Pro Preview, one of the newest advanced models, and reflect on how AI systems have evolved during this period. More importantly, I'll examine whether they're fulfilling their initial promise for security threat modeling applications.

{{< figure src="https://xvnpw.github.io/threat-modeling-with-llms-two-years-in-hype-hope-and-a-look-at-gemini-2.5-pro.png.png" class="image-center" width=400 >}}

## The Experiment: Testing Gemini 2.5 Pro

As with my previous research, the goal is to assess how effectively LLMs can assist in threat modeling. For this, I used a deliberately underspecified architecture for a fictional project, "AI Nutrition Pro".

{{< figure src="https://xvnpw.github.io/ai-nutrition-pro-arch.png" class="image-center" width=400 caption="Architecture diagram for the fictional AI Nutrition Pro project." >}}

You can find the [architecture description here](https://github.com/xvnpw/ai-security-analyzer/blob/b7a9abde4f790da10c78f936d8e2161c125dfd4f/tests/EXAMPLE_ARCHITECTURE.md). I intentionally left it lacking detail to see how well the LLM could handle ambiguity and missing information.

### Methodology: AI Security Analyzer and Prompting

To conduct the threat modeling, I utilized my open-source tool, [AI Security Analyzer](https://github.com/xvnpw/ai-security-analyzer). The tool operates in *file* mode, reading the architecture description from a file and then running a specific prompt against the chosen LLM to generate a threat model in Markdown format.

The command looks like this:
```bash
python ai_security_analyzer/app.py \
    file \
    -t tests/EXAMPLE_ARCHITECTURE.md \
    -o examples/threat-modeling.md \
    --agent-model $agent_model \
    --agent-temperature ${temperatures[$agent_model]} \
    --agent-prompt-type threat-modeling \
    --agent-provider $agent_provider
```

Check [create_examples.sh](https://github.com/xvnpw/ai-security-analyzer/blob/main/scripts/create_examples.sh) for more details.

The prompt I used is based on the STRIDE per element methodology and guides the LLM through the necessary steps to create a comprehensive threat model. While it supports refinement iterations, this feature wasn't used in this specific experiment. You can view the [full prompt structure here](https://github.com/xvnpw/ai-security-analyzer/blob/b7a9abde4f790da10c78f936d8e2161c125dfd4f/ai_security_analyzer/prompts/default/file/threat-modeling-default.yaml).

A snippet of the prompt's core instruction:
```markdown
# IDENTITY and PURPOSE

You are an expert in risk and threat management and cybersecurity. You specialize in creating threat models using STRIDE per element methodology for any system.

# GOAL

Given a FILE and CURRENT THREAT MODEL, provide a threat model using STRIDE per element methodology.

# STEPS

- Think deeply about the input and what they are concerned with.

- Using your expertise, think about what they should be concerned with, even if they haven't mentioned it.

- Fully understand the STRIDE per element threat modeling approach.

- If CURRENT THREAT MODEL is not empty - it means that draft of this document was created in previous interactions with LLM using FILE content. In such case update CURRENT THREAT MODEL with new information that you get from FILE. In case CURRENT THREAT MODEL is empty it means that you first time get FILE content

- Take the input provided and create a section called APPLICATION THREAT MODEL.

- Under that, create a section called ASSETS, take the input provided and determine what data or assets need protection. List and describe those.

- Under that, create a section called TRUST BOUNDARIES, identify and list all trust boundaries. Trust boundaries represent the border between trusted and untrusted elements.

... (further steps omitted for brevity)
```

### A Note on Limitations

A potential limitation is that all previous threat models from my experiments are publicly available in a GitHub repository. It's possible that the AI models I'm testing have been trained on this data. In the future, I aim to prepare a new, unpublished dataset, perhaps inspired by projects like [TM-Bench](https://www.tmbench.com/), to mitigate this.

### Results: Gemini 2.5 Pro Preview

I'll focus my comments on the "Application Threat Model" section generated by Gemini 2.5 Pro Preview.
You can find the [full, detailed results here](https://github.com/xvnpw/ai-security-analyzer/blob/main/examples/file-threat-modeling-ai-nutrition-pro-gemini-2.5-pro-preview-05-06.md).

**Application Threat Model Highlights (Gemini 2.5 Pro Preview):**

| Item | Valid | Comment |
| --- | --- | --- |
| Assets | ✅ | Well-described and *completely valid* assets. |
| Trust Boundaries | ❌ | While not entirely incorrect, I wouldn't typically define trust boundaries for internal services in this manner. |
| Data Flows | ✅ | Well-described and *completely valid* data flows. |
| **Threat:** Stolen Meal Planner API key used to impersonate a legitimate client. | ✅ | Really like how it's described. Model mentioned what is currently implemented as mitigation and what is missing. |
| **Threat:** Prompt injection attack modifies LLM behavior leading to unintended output or data leakage. | ✅ | Well-described and *completely valid* threat. |
| **Threat:** Attacker bypasses ACL rules due to misconfiguration or overly permissive rules. | ✅ | Nicely captured possible misconfiguration (which can easily happened) and how it can be mitigated. |
| **Threat:** Administrator credentials compromised, leading to unauthorized access to administrative functions. | ✅ | Correctly identified a critical threat. I omitted mitigations like MFA in the architecture to test the model's handling of such gaps. |
| **Threat:** Unauthorized access to dietitian's content samples or LLM interaction logs via SQL injection in API Application. | ✅ | Some might argue SQL injection is less relevant for modern apps, but the architecture lacked explicit mitigations (e.g., SAST, DevSecOps practices). Ranked as medium. |
| **Threat:** Excessive API calls to ChatGPT leading to high operational costs or rate limiting from OpenAI. | ✅ | Well-described and *completely valid* threat. |
| **Threat:** Unauthorized access to tenant, billing, or configuration data via SQL injection in Web Control Plane. | ✅ | Similar reasoning as the API Application SQL injection threat; valid given the input. |
| **Threat:** Sensitive data within prompts sent to ChatGPT is exposed due to ChatGPT's data handling or a breach at OpenAI. | ✅ | Interesting mitigations that include: "Anonymize or pseudonymize data where possible". |
| **Threat:** Insecure direct object references (IDOR) allowing one Meal Planner to access/modify another's data. | ✅ | Correctly noted that the API Gateway's ACLs partially mitigate this, but also rightly suggested data ownership validation in the API Application. |

**Key Observation on Gemini 2.5 Pro Preview:**
Gemini 2.5 Pro Preview produced the most impressive threat model I've seen to date. I was particularly struck by the quality of the mitigations suggested. I suspect its performance might be linked to its nature as a hybrid model, potentially excelling in reasoning tasks.

For those interested in comparing, results from other models are available in the [examples folder](https://github.com/xvnpw/ai-security-analyzer/blob/main/examples/README.md#file-mode) of the repository.

## Reassessing AI's Promise in Threat Modeling

When I began this research journey, I was genuinely excited about AI's potential to automate threat modeling. Two years later, I've developed a more nuanced perspective on what's actually achievable.

### Understanding LLM Capabilities and Limitations

Large Language Models use neural networks to process and generate human language with impressive results. Their ability to find solutions for problems that would be nearly impossible to solve with deterministic tools is remarkable.

However, can they truly replace human intelligence in security analysis? My research suggests not yet.

### Comparison with Traditional Automation Tools

Traditional threat modeling tools like [pytm](https://github.com/OWASP/pytm) operate on predefined rules and conditions:

```python
isPII=False,
isCredentials=False,
classification=Classification.PUBLIC,
...
```

These tools require us to formally describe our architecture using their specific syntax.

LLMs offer a significant advantage here: they understand natural language (and increasingly, visual information), allowing us to provide architecture descriptions without specialized syntax. This significantly lowers the barrier to entry.

In one of my early [experiments](https://github.com/xvnpw/ai-threat-modeling/blob/0cbb5af1349e6831f09fddc16e9e47cfea0fcd02/architecture.py#L95), I explored a pipeline approach - breaking the process into smaller steps by first asking the LLM about threat modeling plan and data flows, then generating threats for each flow separately. This approach seems more promising for achieving consistent, high-quality results than single massive prompts.

### Refined Expectations for LLMs in Threat Modeling (Mid-2025)

Based on my research, here are my current views on leveraging LLMs for threat modeling:

- ⭐️⭐️ **Learning Aid**: LLMs significantly flatten the learning curve for threat modeling newcomers.
- ⭐️⭐️ **Idea Generation**: LLMs excel as inspiration tools. Developers can use them to create initial threat models before consulting security specialists. Matthew Adams highlights this approach in his [STRIDE GPT presentation](https://www.youtube.com/watch?v=q5hMF46vgWE).
- ⭐️ **Rule-Based Alternative**: LLMs might replicate deterministic tools like `pytm` when using a decomposed approach with smaller, focused prompts. Think of LLMs as having limited brain. They handle simple tasks reasonably well but struggle with complex reasoning that requires maintaining numerous dependencies. While they occasionally produce surprisingly good results (much like our pets can surprise us!), **consistent performance** remains elusive.
- ⭐️⭐️ **Context Enrichment**: LLMs can identify threats and risks implied by context. It's something deterministic tools cannot do. This capability extends beyond threat modeling to vulnerability research, as discussed in my previous post: [Can AI Actually Find Real Security Bugs? Testing the New Wave of AI Models]({{< ref "/posts/can-ai-actually-find-real-security-bugs-testing-the-new-wave-of-ai-models.md" >}})
- ⭐️ **Domain-Specific Training**: Training LLMs on architecture examples paired with expert-created threat models could improve their generalization capabilities. Threat modeling is a verifiable domain where we can score model performance and incorporate feedback into the learning process.
- ❌ **Full Automation**: I remain skeptical about using LLMs to completely automate threat modeling, especially using single-step, monolithic prompts.

## Conclusion

After examining dozens of AI-generated threat models over the past six months, I've noticed my enthusiasm waning. Not because the results are poor, but because I've developed a more realistic view of their capabilities and limitations.

How do I reconcile my cautious perspective with the impressive results from Gemini 2.5 Pro Preview? While the model performed exceptionally well on my test case, I question whether such performance would generalize across diverse scenarios. A comprehensive benchmark like the one mentioned earlier could help answer this question.

To be clear: LLMs can absolutely enhance threat modeling workflows today. We don't need to wait for bigger, better models. We just need to understand their current limitations and design workflows accordingly.

Looking forward, I wonder which approach will bring us closer to truly effective threat modeling automation:

- **Larger, more capable models?** Since ChatGPT's emergence, I've anticipated models that could consume an organization's entire context (repositories, JIRA tickets, Confluence documentation) and deliver reliable, consistent answers. Are we there yet? My experience suggests not. Even recent tests of Microsoft Copilot with SharePoint for HR inquiries in my organization yielded frequent inaccuracies.
- **Smaller, faster, specialized models?** These could support the decomposed prompt approach and potentially serve as a foundation for AI-enhanced tools like Jira, Confluence, and GitHub. Such models could continuously evaluate targeted contexts and generate intermediate data that reflects company-specific factors when creating threat models.
- **Multi-agent systems with specialized tools?** While agents might improve LLM reliability similar to Chain of Thought (CoT) or reasoning capabilities, they may not be the ultimate solution. An agent creating a threat model could search various systems for additional information, but this might result in context overload for the LLM's limited reasoning capacity.

I'm interested in your thoughts on these approaches. You can most easily reach me on [LinkedIn](https://www.linkedin.com/in/marcin-niemiec-304349104/).

---

Thanks for reading! You can contact me and/or follow me on [X](https://x.com/xvnpw) and [LinkedIn](www.linkedin.com/in/marcin-niemiec-304349104).
