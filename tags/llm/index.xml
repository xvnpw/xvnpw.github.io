<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Llm on xvnpw personal blog</title><link>https://xvnpw.github.io/tags/llm/</link><description>Recent content in Llm on xvnpw personal blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 19 Dec 2024 08:59:02 +0100</lastBuildDate><atom:link href="https://xvnpw.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>AI Security Analyzer - All-in-One Tool Preview</title><link>https://xvnpw.github.io/posts/ai-security-analyzer-all-in-one-tool-preview/</link><pubDate>Thu, 19 Dec 2024 08:59:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/ai-security-analyzer-all-in-one-tool-preview/</guid><description>&lt;p>&lt;a href="https://github.com/xvnpw/ai-security-analyzer">AI Security Analyzer&lt;/a> is my latest project - a powerful tool that leverages AI to automatically generate comprehensive security documentation for your projects. Whether you&amp;rsquo;re dealing with security design, threat modeling, attack surface analysis, or more, this tool aims to simplify and enhance your security documentation process.&lt;/p>
&lt;figure class="image-center">&lt;img src="https://github.com/user-attachments/assets/86ebd729-24ef-48cf-b704-3f42a8e34162" width="200">
&lt;/figure>

&lt;p>&lt;strong>Watch the demo:&lt;/strong>
&lt;figure class="image-center">&lt;img src="https://github.com/user-attachments/assets/a9de6ce7-9702-4fae-97a4-424d03a683eb">
&lt;/figure>
&lt;/p>
&lt;h2 id="threats-vs-vulnerabilities">Threats vs. Vulnerabilities&lt;/h2>
&lt;p>Before we dive in, I want to clarify an important distinction: &lt;strong>threats&lt;/strong> vs. &lt;strong>vulnerabilities&lt;/strong>. My tool focuses on identifying threats, which are &lt;strong>potential risks&lt;/strong> that could be exploited by an attacker. Vulnerabilities, on the other hand, are specific weaknesses in a system that can be exploited. If you&amp;rsquo;re looking for tools to identify vulnerabilities, I recommend checking out &lt;a href="https://googleprojectzero.blogspot.com/2024/10/from-naptime-to-big-sleep.html">Google Project Zero&lt;/a>.&lt;/p></description></item><item><title>Automating GitHub Workflows with Fabric Agent Action</title><link>https://xvnpw.github.io/posts/automating-github-workflows-with-fabric-agent-action/</link><pubDate>Fri, 22 Nov 2024 13:59:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/automating-github-workflows-with-fabric-agent-action/</guid><description>&lt;p>&lt;a href="https://github.com/xvnpw/fabric-agent-action">Fabric Agent Action&lt;/a> is a GitHub Action that bridges the gap between &lt;a href="https://github.com/danielmiessler/fabric">fabric&lt;/a> patterns and GitHub workflows. Instead of manually executing patterns or building custom integrations, you can automate complex tasks directly in your GitHub workflows using an agent-based approach.&lt;/p>
&lt;figure class="image-center">&lt;img src="https://github.com/user-attachments/assets/14110f11-1250-4792-8d1a-4da3fd85197e" width="200">
&lt;/figure>

&lt;p>The action introduces different types of agents, each designed for specific use cases, from simple pattern execution to complex reasoning about GitHub issues and pull requests. Let&amp;rsquo;s explore how these agents work and evaluate their effectiveness.&lt;/p></description></item><item><title>Create design documents with Fabric</title><link>https://xvnpw.github.io/posts/fabric_design_documents/</link><pubDate>Wed, 30 Oct 2024 11:59:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/fabric_design_documents/</guid><description>&lt;p>I encountered a challenge in creating high-quality design documents for my threat modeling research. About a year and a half ago, I created &lt;a href="https://github.com/xvnpw/ai-nutrition-pro-design-gpt3.5/blob/main/ARCHITECTURE.md">AI Nutrition-Pro&lt;/a> architecture and have been using it since then. What if it&amp;rsquo;s already in LLMs&amp;rsquo; training data? Testing threat modeling capabilities could give me false results.&lt;/p>
&lt;p>I developed several prompts to assist with the challenging task of creating design documents. I implemented these as &lt;a href="https://github.com/danielmiessler/fabric">Fabric&lt;/a> patterns for everyone&amp;rsquo;s benefit. If you&amp;rsquo;re unfamiliar with Fabric - it&amp;rsquo;s an excellent CLI tool created by &lt;a href="https://danielmiessler.com">Daniel Miessler&lt;/a>.&lt;/p></description></item><item><title>Threat Modelling with Fabric Framework</title><link>https://xvnpw.github.io/posts/threat_modelling_with_fabric_framework/</link><pubDate>Mon, 03 Jun 2024 16:59:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/threat_modelling_with_fabric_framework/</guid><description>&lt;p>&lt;a href="https://github.com/danielmiessler/fabric">Fabric&lt;/a> is a framework that puts AI at your fingertips. Instead of diving into chat interfaces (e.g., ChatGPT) or writing custom programs that consume APIs, you can create prompts as markdown text and receive output in markdown. Fabric also maintains a database of prompts called &lt;a href="https://github.com/danielmiessler/fabric/tree/main/patterns">patterns&lt;/a>.&lt;/p>
&lt;figure class="image-center">&lt;img src="https://github.com/xvnpw/xvnpw.github.io/assets/17719543/4fe91d36-3736-4cbf-9835-edfa3943116e" width="300">
&lt;/figure>

&lt;p>With the new pattern &lt;a href="https://github.com/danielmiessler/fabric/blob/main/patterns/create_stride_threat_model/system.md">create_stride_threat_model&lt;/a>, you can easily create threat models. Let&amp;rsquo;s dive deeper into how to use this new pattern and evaluate the quality of the results.&lt;/p></description></item><item><title>Leveraging LLMs for Threat Modeling - Claude 3 Opus vs GPT-4</title><link>https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-claude-3-vs-gpt-4/</link><pubDate>Wed, 20 Mar 2024 14:59:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-claude-3-vs-gpt-4/</guid><description>&lt;p>&lt;a href="https://www.anthropic.com/news/claude-3-family">Claude 3 Opus&lt;/a> is the latest and most powerful model from Anthropic. Is it able to overcome GPT-4?&lt;/p>
&lt;figure class="image-center">&lt;img src="https://github.com/xvnpw/xvnpw.github.io/assets/17719543/3305cef0-c07d-4239-8fd9-c6e9e14146e7">
&lt;/figure>

&lt;h2 id="revisiting-the-experiment">Revisiting the Experiment&lt;/h2>
&lt;p>If you wish to understand more about the experiment structure, you can refer to my &lt;a href="https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5/">previous post&lt;/a>. But here&amp;rsquo;s a quick recap:&lt;/p>
&lt;p>I used markdown files describing a fictional project, &lt;strong>AI Nutrition-Pro&lt;/strong>, with input:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/xvnpw/ai-nutrition-pro-design-claude3-opus/blob/main/PROJECT.md">PROJECT.md&lt;/a> - high level project description&lt;/li>
&lt;li>&lt;a href="https://github.com/xvnpw/ai-nutrition-pro-design-claude3-opus/blob/main/ARCHITECTURE.md">ARCHITECTURE.md&lt;/a> - architecture description&lt;/li>
&lt;li>&lt;a href="https://github.com/xvnpw/ai-nutrition-pro-design-claude3-opus/blob/main/user-stories/0001_STORE_DIET_INTRODUCTIONS.md">0001_STORE_DIET_INTRODUCTIONS.md&lt;/a> - user story&lt;/li>
&lt;/ul>
&lt;p>I tasked the AI models with four types of analysis: high-level security design review, threat modeling, security-related acceptance criteria and review of architecture:&lt;/p></description></item><item><title>Reviewing Your Architecture Using LLMs</title><link>https://xvnpw.github.io/posts/review_your_architecture_using_llms/</link><pubDate>Wed, 25 Oct 2023 08:59:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/review_your_architecture_using_llms/</guid><description>&lt;p>Recently, I&amp;rsquo;ve discussed &lt;a href="https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5-vs-claude2-vs-gpt-4/">utilizing LLMs for threat modeling&lt;/a>. Building on that, I&amp;rsquo;ve incorporated a new feature in my &lt;a href="https://github.com/xvnpw/ai-threat-modeling-action">ai-threat-modeling-action&lt;/a> on GitHub. It now allows you to review your project&amp;rsquo;s architectural description using LLMs. Let&amp;rsquo;s delve deeper ðŸ¤¿!&lt;/p>
&lt;h2 id="revisiting-the-experiment">Revisiting the Experiment&lt;/h2>
&lt;p>For a detailed understanding of the experiment&amp;rsquo;s structure, you can revisit my &lt;a href="https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5/">previous post&lt;/a>. However, here&amp;rsquo;s a brief summary:&lt;/p>
&lt;p>I utilized markdown files, which described a fictional project named &lt;strong>AI Nutrition-Pro&lt;/strong>, as input:&lt;/p></description></item><item><title>Leveraging LLMs for Threat Modeling - GPT-3.5 vs Claude 2 vs GPT-4</title><link>https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5-vs-claude2-vs-gpt-4/</link><pubDate>Sun, 03 Sep 2023 08:59:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5-vs-claude2-vs-gpt-4/</guid><description>&lt;p>In a bid to uncover which AI model is best at threat modeling, I put GPT-3.5, Claude 2, and GPT-4 to the test. Let&amp;rsquo;s see how they performed.&lt;/p>
&lt;figure class="image-center">&lt;img src="https://github.com/xvnpw/xvnpw.github.io/assets/17719543/bf036c66-1d66-4468-8dcd-426d6e0f40f6">
&lt;/figure>

&lt;p>&lt;a href="https://twitter.com/emollick">Ethan Mollick&lt;/a>, a professor at The Wharton School, once said something that perfectly captures my experience with these AI models. It feels as if they&amp;rsquo;re striving to answer questions in the simplest way possible ðŸ˜ However, you can get around this by asking for detailed explanations or step-by-step thinking.&lt;/p></description></item><item><title>Leveraging LLMs for Threat Modeling - GPT-3.5</title><link>https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5/</link><pubDate>Thu, 17 Aug 2023 18:59:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5/</guid><description>&lt;p>In this article, I delve into the &lt;a href="https://github.com/xvnpw/ai-nutrition-pro-design-gpt3.5">AI Nutrition-Pro experiment&lt;/a>, a research project exploring the potential of LLMs in enhancing security practices during the design phase of DevSecOps: &lt;strong>threat modeling&lt;/strong> and &lt;strong>security review&lt;/strong>.&lt;/p>
&lt;h2 id="devsecops-a-brief-overview">DevSecOps: A Brief Overview&lt;/h2>
&lt;p>DevSecOps merges the principles of development, security, and operations to create a culture of shared responsibility for software security. The three main goals of DevSecOps are:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Shift Left Security:&lt;/strong> Identifying and addressing security vulnerabilities as early as possible in the software development lifecycle.&lt;/li>
&lt;li>&lt;strong>Developer-Centric:&lt;/strong> Integrating security practices seamlessly into the developer&amp;rsquo;s ecosystem, including Integrated Development Environments (IDEs), code hosting platforms, and pull requests.&lt;/li>
&lt;li>&lt;strong>Fast Feedback and Guidance:&lt;/strong> Providing developers with rapid feedback on security issues and guidance on secure coding practices.&lt;/li>
&lt;/ul>
&lt;p>While security tools like &lt;a href="https://semgrep.dev/blog/2023/using-ai-to-write-secure-code-with-semgrep">semgrep&lt;/a> can already use LLMs in the coding phase, the AI Nutrition-Pro experiment seeks to explore the benefits of LLMs during the design phase, particularly in security design reviews and threat modeling.&lt;/p></description></item></channel></rss>