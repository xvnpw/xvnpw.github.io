<!doctype html><html><head lang=en><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Scaling Threat Modeling with AI: Generating 1000 Threat Models Using Gemini 2.0 and AI Security Analyzer - xvnpw personal blog</title><link rel=icon type=image/png href=/favicon.png><meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="An in-depth look at how I leveraged Gemini 2.0 to create a massive security documentation repository, complete with practical examples and lessons learned."><meta property="og:image" content><meta property="og:url" content="https://xvnpw.github.io/posts/scaling-threat-modeling-with-ai/"><meta property="og:site_name" content="xvnpw personal blog"><meta property="og:title" content="Scaling Threat Modeling with AI: Generating 1000 Threat Models Using Gemini 2.0 and AI Security Analyzer"><meta property="og:description" content="An in-depth look at how I leveraged Gemini 2.0 to create a massive security documentation repository, complete with practical examples and lessons learned."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-01T12:19:02+01:00"><meta property="article:modified_time" content="2025-01-01T12:19:02+01:00"><meta property="article:tag" content="Security"><meta property="article:tag" content="Llm"><meta property="article:tag" content="Gpt"><meta property="article:tag" content="Ai"><meta property="article:tag" content="Github"><meta property="article:tag" content="Langchain"><meta name=twitter:card content="summary"><meta name=twitter:title content="Scaling Threat Modeling with AI: Generating 1000 Threat Models Using Gemini 2.0 and AI Security Analyzer"><meta name=twitter:description content="An in-depth look at how I leveraged Gemini 2.0 to create a massive security documentation repository, complete with practical examples and lessons learned."><link href=https://xvnpw.github.io/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://xvnpw.github.io/css/main.6a0f23ea50fd34b46fee262a5a68e17d458c51a2bc99ba1ba018065de6b180c3.css><link id=darkModeStyle rel=stylesheet type=text/css href=https://xvnpw.github.io/css/dark.50b57e12d401420df23965fed157368aba37b76df0ecefd0b1ecd4da664f01a0.css media="(prefers-color-scheme: dark)"><link rel=stylesheet type=text/css href=https://xvnpw.github.io/css/my.6e08ae20ebbcf10b8953bc0c3935a825ae172c99096f9af1bf2df91a6d21a1be.css></head><body><div class=content><header><div class=main><a href=https://xvnpw.github.io/>xvnpw personal blog</a></div><nav><a href=/>Home</a>
<a href=/posts>All posts</a>
<a href=/about>About</a>
<a href=/tags>Tags</a></nav></header><main><article><div class=post-container><div class=post-content><div class=title><h1 class=title>Scaling Threat Modeling with AI: Generating 1000 Threat Models Using Gemini 2.0 and AI Security Analyzer</h1><div class=meta>Posted on Jan 1, 2025</div></div><section class=body><p>&ldquo;Can AI help us scale security analysis?&rdquo; This question led me down a fascinating path of experimenting with Google&rsquo;s Gemini 2.0 to generate threat models at an unprecedented scale. In this post, I&rsquo;ll share how I turned this ambitious idea into reality, complete with code samples, real outputs, and valuable lessons learned along the way.</p><h2 id=the-challenge-automating-security-analysis-at-scale>The Challenge: Automating Security Analysis at Scale</h2><p>Security documentation is crucial but often becomes a bottleneck in fast-moving development cycles. With the release of Google&rsquo;s Gemini 2.0 Flash Thinking Experimental model, I saw an opportunity to tackle this challenge head-on, despite some notable limitations:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Model Constraints:
</span></span><span style=display:flex><span>✗ 8k output token limit
</span></span><span style=display:flex><span>✗ Markdown formatting issues
</span></span><span style=display:flex><span>✗ Knowledge cutoff (Oct 2023)
</span></span></code></pre></div><p>The model presented an opportunity due to its &ldquo;thinking&rdquo; capabilities, utilizing chain-of-thought reasoning to tackle complex problems. And it&rsquo;s free 💸&mldr;</p><h2 id=the-quest-to-leverage-gemini-for-threat-modeling>The Quest to Leverage Gemini for Threat Modeling</h2><p>Building upon my previous work like <a href=https://xvnpw.github.io/posts/threat_modelling_with_fabric_framework/>Threat Modeling with Fabric Framework</a>, I&rsquo;ve been a long-time advocate for the STRIDE methodology. I had a finely tuned <a href=https://github.com/danielmiessler/fabric/blob/main/patterns/create_stride_threat_model/system.md>prompt</a> that performed exceptionally with OpenAI&rsquo;s o1-preview model. Naturally, I was eager to see how it fared with Gemini 2.0.</p><h2 id=hitting-the-initial-roadblocks>Hitting the Initial Roadblocks</h2><p>To my surprise, Gemini didn&rsquo;t play well with my existing prompts:</p><ul><li><strong>Inconsistent Markdown Generation</strong>: The model struggled to produce valid and consistent Markdown, which was crucial for documentation.</li><li><strong>Less Effective Prompts</strong>: My go-to prompts yielded erratic results, lacking the determinism I needed.</li></ul><p>It became clear that I couldn&rsquo;t just copy and paste my methods — I had to rethink my approach.</p><h2 id=the-journey-to-better-prompts>The Journey to Better Prompts</h2><p>My initial attempts were&mldr; interesting, to say the least. Simple prompts like &ldquo;Create threat model for X&rdquo; produced wildly inconsistent results, while complex prompts often led to the AI equivalent of a deer in headlights.</p><p>Here&rsquo;s what I learned about prompt engineering through trial and error:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Effectiveness Spectrum:
</span></span><span style=display:flex><span>Too Simple        &lt;---|--------------|---&gt; Too Complex
</span></span><span style=display:flex><span>&#34;Create threat model&#34; |  Sweet Spot  | Multi-page instructions
</span></span><span style=display:flex><span>                             ↑
</span></span><span style=display:flex><span>                    Where magic happens
</span></span></code></pre></div><p>I needed a balanced approach — a &ldquo;sweet spot&rdquo; where the prompts were sufficiently detailed to guide the model but not so complex that they overwhelmed it.</p><h2 id=crafting-effective-prompts-the-game-changer>Crafting Effective Prompts: The Game Changer</h2><p>The breakthrough came when I shifted from single, complex prompts to a sequence of targeted prompts. I developed a new agent, <a href=https://github.com/xvnpw/ai-security-analyzer/blob/main/ai_security_analyzer/github2_agents.py>Github2Agent</a>, to facilitate a multi-turn conversation with Gemini.</p><h2 id=the-multi-step-prompt-strategy>The Multi-Step Prompt Strategy</h2><p>Here&rsquo;s the refined set of prompts:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>GITHUB2_THREAT_MODELING_PROMPTS <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;You are cybersecurity expert, working with development team. Your task is to create threat model for application that is using </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>. Focus on threats introduced by </span><span style=color:#e6db74>{}</span><span style=color:#e6db74> and omit general, common web application threats. Use valid markdown formatting. Don&#39;t use markdown tables at all, use markdown lists instead.&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Create threat list with: threat, description (describe what the attacker might do and how), impact (describe the impact of the threat), which </span><span style=color:#e6db74>{}</span><span style=color:#e6db74> component is affected (describe what component is affected, e.g. module, function, etc.), risk severity (critical, high, medium or low), and mitigation strategies (describe how can developers or users reduce the risk). Use valid markdown formatting. Don&#39;t use markdown tables at all, use markdown lists instead.&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Update threat list and return only threats that directly involve </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>. Return high and critical threats only. Use valid markdown formatting. Don&#39;t use markdown tables at all, use markdown lists instead.&#34;</span>,
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p><strong>Key elements of this strategy:</strong></p><ul><li><strong>Sequential Guidance</strong>: Each prompt builds upon the previous response, allowing the model to refine and focus its output incrementally.</li><li><strong>Dynamic Placeholders</strong>: The <code>{}</code> placeholders are dynamically replaced with the specific GitHub repository URL and name, tailoring the prompts to each project. I don&rsquo;t analyze code from the repository but rely on the model&rsquo;s knowledge 🧠 (by the way, AI Security Analyzer is able to analyze code from the repository in <code>dir</code> mode).</li><li><strong>Focused Filtering</strong>: By instructing the model to &ldquo;Return only high and critical threats,&rdquo; we ensure the output is relevant and actionable.</li></ul><h2 id=an-example-output>An Example Output</h2><p>Applying this approach to the Python <strong>Flask</strong> framework yielded promising results:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span>Here are the high and critical threats that directly involve the Flask framework:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Threat:**</span> Route Parameter Injection
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Description:**</span> An attacker manipulates route parameters within the URL. Flask&#39;s routing mechanism allows for dynamic parameters, and if these are not properly validated, attackers can inject malicious input to access unintended resources or trigger unexpected application behavior. This could involve path traversal or injecting commands.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Impact:**</span> Unauthorized access to data or functionality, potentially leading to data breaches, privilege escalation, or denial of service.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Affected Flask Component:**</span> <span style=color:#e6db74>`flask.Flask.route`</span>, <span style=color:#e6db74>`flask.request.view_args`</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Risk Severity:**</span> High
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Mitigation Strategies:**</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>*</span>   Implement strict input validation and sanitization for all route parameters.
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>*</span>   Avoid directly using route parameters in file system operations or other sensitive actions without thorough checks.
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>*</span>   Utilize type converters in route definitions to enforce expected data types.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Threat:**</span> Insecure Session Cookie Configuration
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Description:**</span> Flask uses signed cookies for session management. If the <span style=color:#e6db74>`SECRET_KEY`</span> configured within the <span style=color:#e6db74>`flask.Flask`</span> application is weak or compromised, attackers can forge session cookies and impersonate legitimate users. Additionally, if the <span style=color:#e6db74>`secure`</span> and <span style=color:#e6db74>`httponly`</span> flags are not properly set for the session cookie by Flask&#39;s session management, it can be vulnerable to interception or client-side scripting attacks.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Impact:**</span> Account takeover, unauthorized access to user data and functionalities.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Affected Flask Component:**</span> <span style=color:#e6db74>`flask.session`</span>, <span style=color:#e6db74>`flask.Flask.secret_key`</span>, Flask&#39;s cookie setting mechanism.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Risk Severity:**</span> Critical
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Mitigation Strategies:**</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>*</span>   Use a strong, randomly generated <span style=color:#e6db74>`SECRET_KEY`</span> and store it securely (e.g., environment variables).
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>*</span>   Configure session cookie flags (<span style=color:#e6db74>`httponly=True`</span>, <span style=color:#e6db74>`secure=True`</span> in production) appropriately, either through Flask configuration or directly when setting the cookie.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Threat:**</span> Debug Mode Enabled in Production
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Description:**</span> Running a Flask application with <span style=color:#e6db74>`debug=True`</span> configures the <span style=color:#e6db74>`flask.Flask`</span> application to expose an interactive debugger in the browser when an error occurs. Attackers can exploit this debugger to execute arbitrary code on the server, access sensitive information, and potentially gain full control of the application.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Impact:**</span> Complete server compromise, data breaches, denial of service.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Affected Flask Component:**</span> <span style=color:#e6db74>`flask.Flask`</span>, <span style=color:#e6db74>`debug`</span> configuration parameter.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Risk Severity:**</span> Critical
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Mitigation Strategies:**</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Never**</span> run Flask applications with <span style=color:#e6db74>`debug=True`</span> in production environments. Ensure <span style=color:#e6db74>`app.debug = False`</span> or the <span style=color:#e6db74>`FLASK_DEBUG=0`</span> environment variable is set.
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>*</span>   Implement proper logging and error reporting mechanisms for production.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Threat:**</span> Blueprint Route Conflicts and Overlapping
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Description:**</span> When using Flask Blueprints to structure an application, developers might unintentionally define routes that overlap or conflict within the <span style=color:#e6db74>`flask.Blueprint`</span> instances or when registering them with the main <span style=color:#e6db74>`flask.Flask`</span> application. An attacker could exploit this by accessing a route intended for a different blueprint, potentially bypassing security checks or accessing unintended functionality.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Impact:**</span> Unexpected application behavior, potential security bypasses, access to unintended resources or functionalities.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Affected Flask Component:**</span> <span style=color:#e6db74>`flask.Blueprint`</span>, <span style=color:#e6db74>`flask.Flask.register_blueprint`</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Risk Severity:**</span> Medium (While listed as medium before, in certain scenarios leading to significant bypasses, it can be High. Let&#39;s keep it as High for this filtered list focusing on direct Flask involvement).
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Mitigation Strategies:**</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>*</span>   Carefully plan and manage route definitions within blueprints.
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>*</span>   Use unique prefixes or subdomains for blueprints to avoid naming collisions.
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>*</span>   Thoroughly test route configurations to identify and resolve any conflicts. Flask provides tools to inspect the registered routes.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Threat:**</span> Incorrect HTTP Method Handling
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Description:**</span> Developers using the <span style=color:#e6db74>`@app.route()`</span> decorator or <span style=color:#e6db74>`add_url_rule()`</span> on the <span style=color:#e6db74>`flask.Flask`</span> application might not correctly restrict the allowed HTTP methods (GET, POST, PUT, DELETE, etc.) for specific routes. An attacker could leverage this by using an unintended method to perform actions they shouldn&#39;t be able to, such as modifying data via a GET request if the route handler doesn&#39;t properly validate the method.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Impact:**</span> Data modification, unauthorized actions, potential security breaches.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Affected Flask Component:**</span> <span style=color:#e6db74>`flask.Flask.route`</span>, <span style=color:#e6db74>`methods`</span> argument in route definition.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Risk Severity:**</span> Medium (Similar to Blueprint conflicts, if leading to critical data modification, it can be High. Let&#39;s keep it as High for this focused list).
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Mitigation Strategies:**</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>*</span>   Explicitly define the allowed HTTP methods for each route using the <span style=color:#e6db74>`methods`</span> argument in the <span style=color:#e6db74>`@app.route()`</span> decorator.
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>*</span>   Implement proper handling for each allowed method and reject requests with other methods.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>These threats directly involve the core components and functionalities provided by the Flask framework itself. Remember to also consider general web application security best practices.
</span></span></code></pre></div><p>The output was surprisingly detailed and relevant, though not perfect. It highlighted critical security concerns while providing actionable mitigation strategies.</p><h2 id=expanding-to-four-document-types>Expanding to Four Document Types</h2><p>Encouraged by the success with threat modeling, I extended the approach to generate four different types of security documents:</p><ul><li>🔒 <strong>Security Design Documentation</strong>: Generating detailed security design review.</li><li>🎯 <strong>Threat Modeling</strong>: Performing threat modeling analysis.</li><li>🔍 <strong>Attack Surface Analysis</strong>: Identifying potential entry points and vulnerabilities in the project&rsquo;s attack surface.</li><li>🌳 <strong>Attack Tree Analysis</strong>: Visualizing potential attack vectors and their hierarchies through attack tree.</li></ul><p>The specific prompts for these documents are defined in the <a href=https://github.com/xvnpw/ai-security-analyzer/blob/dabfc57b6e5da9d99b3df5229fd496a224dac862/ai_security_analyzer/prompts.py#L763>prompts.py</a> file.</p><h2 id=scaling-to-1000-the-infrastructure>Scaling to 1000: The Infrastructure</h2><p>To achieve the goal of 1000 threat models, I needed more than just good prompts. I built a pipeline using GitHub Actions that could:</p><ol><li>Queue and process repositories</li><li>Generate four types of security documentation using my <a href=https://github.com/xvnpw/ai-security-analyzer>AI Security Analyzer</a></li></ol><h2 id=organizing-the-results>Organizing the Results</h2><p>To keep things navigable, the repository is structured by programming language, with folders for each major project. Each project contains subfolders with detailed analyses, organized by date and the specific LLM model used.</p><p><strong>The full list of projects analyzed is <a href=https://github.com/xvnpw/sec-docs/blob/main/.data/origin_repos.txt>available here</a>.</strong> This compilation draws from:</p><ul><li><strong>Generative AI Suggestions</strong>: Enhanced with manual curation to ensure relevance.</li><li><strong><a href=https://github.com/EvanLi/Github-Ranking/tree/master>GitHub Rankings</a></strong>: Leveraging popularity metrics to select impactful projects.</li><li><strong><a href=https://github.com/ossf/wg-securing-critical-projects/tree/main/Initiatives/Identifying-Critical-Projects/Version-1.1>Set of Critical Open Source Projects</a></strong>: Identifying critical projects that are important for the security of the software supply chain.</li></ul><h2 id=reflecting-on-the-journey-evaluating-the-results>Reflecting on the Journey: Evaluating the Results</h2><p>While I haven&rsquo;t yet conducted an exhaustive review of all 1,000 threat models 😅, initial assessments are promising. The methodology demonstrates significant potential in producing valuable security documentation at scale. Some interesting patterns emerged:</p><ul><li><strong>Holistic Insights</strong>: Having all four documents provides different perspectives and insights. Individually, none are perfect, but together they offer a comprehensive view of the project&rsquo;s security posture.</li><li><strong>Solid Threats</strong>: The threats identified are solid — not mind-blowing, but relevant and actionable.</li><li><strong>Knowledge Base Limitations</strong>: Relying on the model&rsquo;s knowledge base is a drawback — we don&rsquo;t always know what version of the code was used for the analysis.</li><li><strong>Focused Priorities</strong>: Concentrating on high and critical threats makes the documents shorter and easier to comprehend.</li></ul><p><strong>Future posts will delve deeper into analysis and refinement.</strong></p><h2 id=closing-thoughts>Closing Thoughts</h2><p>This experiment has been both challenging and thrilling. It highlights the transformative potential of AI models like Gemini 2.0 in automating and scaling critical cybersecurity processes. I would definitely consider using the generated documentation when working on new technology. It serves as a valuable starting point — not replacing human security experts, but making it easier to navigate complexities.</p><p>I would love to generate 1,000 threat models using different LLM models, especially the final version of Gemini 2.0 and the new OpenAI o1, o1-pro, and the upcoming o3.</p><h2 id=want-to-try-it-yourself>Want to Try It Yourself?</h2><p><strong>I invite you to explore the <a href=https://github.com/xvnpw/sec-docs>sec-docs</a> repository.</strong> Review the generated documents, scrutinize the analyses, and share your insights. Your feedback is crucial in refining this approach and enhancing the quality of automated security assessments.</p><hr><p>Thanks for reading! You can contact me and/or follow me on <a href=https://x.com/xvnpw>X</a> and <a href=www.linkedin.com/in/marcin-niemiec-304349104>LinkedIn</a>.</p></section><div class=post-tags><nav class="nav tags"><ul class=tags><li><a href=/tags/security>security</a></li><li><a href=/tags/llm>llm</a></li><li><a href=/tags/gpt>gpt</a></li><li><a href=/tags/ai>ai</a></li><li><a href=/tags/github>github</a></li><li><a href=/tags/langchain>langchain</a></li><li><a href=/tags/gemini>gemini</a></li></ul></nav></div></div></div></article></main><footer><div style=display:flex><a class=soc href=https://github.com/xvnpw rel=me title=GitHub><svg class="feather"><use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#github"/></svg></a><a class=border></a><a class=soc href=https://twitter.com/xvnpw rel=me title=Twitter><svg class="feather"><use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#twitter"/></svg></a><a class=border></a><a class=soc href=https://www.linkedin.com/in/marcin-niemiec-304349104/ rel=me title=Linkedin><svg class="feather"><use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#linkedin"/></svg></a><a class=border></a></div><div class=footer-info>2025 <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a></div></footer><script async src="https://www.googletagmanager.com/gtag/js?id=G-YHETMXZXMZ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-YHETMXZXMZ")}</script></div></body></html>