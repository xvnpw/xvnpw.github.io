---
title: "Sonnet 3.7 and New Vulnerability Workflow"
date: 2025-03-01T16:00:00+01:00
draft: true
tags: [security, ai, vulnerabilities, screenshot-to-code, github, llm]
description: "Showcasing a new vulnerability workflow using Sonnet 3.7 and a custom prompt."
---

With release new model from Anthropic, Sonnet 3.7, I've updated my [ai-security-analyzer](https://github.com/xvnpw/ai-security-analyzer) to use the new model. I also tested it as same benchmark as other models in my previous [post]({{< ref "posts/can-ai-actually-find-real-security-bugs-testing-the-new-wave-of-ai-models.md" >}}).

## How to use Sonnet 3.7

To use Sonnet 3.7, you can run the following command:


```bash
python ai_security_analyzer/app.py \
    dir \
    -t ../path/to/project \
    -o vulnerabilities.md \
    --agent-prompt-type vulnerabilities \
    --agent-provider openrouter \ # or anthropic
    --agent-model anthropic/claude-3.7-sonnet:thinking \ # or claude-3.7-sonnet-latest
    --agent-temperature 1
```

## The Experiment: Testing Against Real Code

### Methodology

I'll run the [prompt](https://github.com/xvnpw/ai-security-analyzer/blob/main/ai_security_analyzer/prompts/default/dir/vulnerabilities-default.yaml) against a project I'm familiar with: [screenshot-to-code](https://github.com/abi/screenshot-to-code). I'll then manually verify each reported vulnerability by attempting to reproduce it. This will give us a sense of the true positive and false positive rates model.

## Results and Analysis

Here's a summary of the results for each model. I've included detailed outputs in the `ai-security-analyzer` repository.

### Anthropic Sonnet 3.7 Thinking

Detailed output: [vulnerabilities.md](https://github.com/xvnpw/ai-security-analyzer/blob/main/examples/dir-vulnerabilities-screenshot-to-code-anthropicclaude-3.7-sonnetthinking.md)

| Vulnerability | Valid | Comment |
| --- | --- | --- |
| Prompt Injection Vulnerability in LLM Requests | ü§î | Well-described vulnerability. Interesting if it's real. Is it possible to do prompt injection by putting phrase on image? |
| Path Traversal in Evaluation Routes | ‚úÖ | Vulnerability is valid. Sonnet 3.7 also correctly identify that it's only possible to read `.html` files. |

## Vulnerability Workflow 1

Each run of tool can give a little bit different results. One time you can get 2 vulnerabilities, another time 3. It's random. So, I decided to run it multiple times and take the best result. I putted all into workflow. Let's see how it works.

### Workflow Explained

Workflow can be described with the following graph:

{{< figure src="https://github.com/user-attachments/assets/54b5fefe-57a6-40ae-9446-eb66d7c62de2"  class="image-center" >}}

Idea is to:
- use two models to find vulnerabilities: primary and secondary,
- use same prompt as for *normal* vulnerability analysis,
- add filter step to remove not relevant vulnerabilities,
- add merge step to get final list of vulnerabilities,

What is the benefit?
- having two different models, should give better results after all,
- what is more interesting, I can use those two models multiple times, getting two, three, four, etc. results for each model,
- all the merging and filtering is also done by LLM, so it's less manual work.

### Reliability

Graph looks promising, but there is catch. For each additional step involving LLM, can bring wrong results. In case of filtering, LLM can remove some valid vulnerabilities. In case of merging, LLM can wrongly merge lists or be lazy and return no results. 

## Testing the Workflow

For testing the workflow, I'll use the same project: `screenshot-to-code`. I will use only one model: Sonnet 3.7 Thinking, but run it multiple times.

### Running `ai-security-analyzer`

You can run it with the following command:

```bash
python ai_security_analyzer/app.py \
    dir \
    -t ../path/to/project \
    -o vulnerabilities.md \
    --agent-prompt-type vulnerabilities-workflow-1 \
    --agent-provider openrouter \ # or anthropic
    --agent-model anthropic/claude-3.7-sonnet:thinking \ # or claude-3.7-sonnet-latest
    --agent-temperature 1
    --vulnerabilities-iterations 8
```

### Results

Detailed output: [vulnerabilities.md](https://github.com/xvnpw/ai-security-analyzer/blob/main/examples/dir-vulnerabilitiesworkflow1-screenshot-to-code-anthropicclaude-3.7-sonnetthinking-i8.md)

| Vulnerability | Valid | Comment |
| --- | --- | --- |
| Cross-Site Scripting (XSS) Through AI-Generated Code | ‚úÖ | Valid vulnerability. If model returns XSS, it will be triggered. |
| Server-Side Request Forgery (SSRF) via Prompt Injection | ‚úÖ | Quite interesting one. Didn't see this before. In case model capability to access network, you might be able to do SSRF, by asking model request to external service. |
| Unrestricted File Upload Leading to Remote Code Execution | ... | Need to verify. |
| Directory Traversal in Evaluation Routes | ‚úÖ | Same as before. It's valid. And model correctly identified that it's only possible to read `.html` files. |
| AI Prompt Injection Vulnerability | ... | Need to verify. |
| CORS Misconfiguration Leading to CSRF and Information Leakage | ‚úÖ | Valid vulnerability. |
| Server-Side Request Forgery (SSRF) via OpenAI Base URL Configuration | ‚ùå | It's don't not look valid. One of precondition is that application is not running on production (IS_PROD=false). |
| Insecure API Key Handling | ‚ùå | Interesting one. This is not really realistic. Description is about tricking model to give back API key. |

