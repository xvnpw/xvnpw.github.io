<!doctype html><html><head lang=en><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>AI Security Analyzer - All-in-One Tool Preview - xvnpw personal blog</title><link rel=icon type=image/png href=/favicon.png><meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="Preview of the AI Security Analyzer - a new tool that leverages AI to automatically generate comprehensive security design documentation for your projects."><meta property="og:image" content><meta property="og:url" content="https://xvnpw.github.io/posts/ai-security-analyzer-all-in-one-tool-preview/"><meta property="og:site_name" content="xvnpw personal blog"><meta property="og:title" content="AI Security Analyzer - All-in-One Tool Preview"><meta property="og:description" content="Preview of the AI Security Analyzer - a new tool that leverages AI to automatically generate comprehensive security design documentation for your projects."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-12-19T08:59:02+01:00"><meta property="article:modified_time" content="2024-12-19T08:59:02+01:00"><meta property="article:tag" content="Security"><meta property="article:tag" content="Llm"><meta property="article:tag" content="Gpt"><meta property="article:tag" content="Ai"><meta property="article:tag" content="Github"><meta property="article:tag" content="Langchain"><meta name=twitter:card content="summary"><meta name=twitter:title content="AI Security Analyzer - All-in-One Tool Preview"><meta name=twitter:description content="Preview of the AI Security Analyzer - a new tool that leverages AI to automatically generate comprehensive security design documentation for your projects."><link href=https://xvnpw.github.io/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://xvnpw.github.io/css/main.6a0f23ea50fd34b46fee262a5a68e17d458c51a2bc99ba1ba018065de6b180c3.css><link id=darkModeStyle rel=stylesheet type=text/css href=https://xvnpw.github.io/css/dark.50b57e12d401420df23965fed157368aba37b76df0ecefd0b1ecd4da664f01a0.css media="(prefers-color-scheme: dark)"><link rel=stylesheet type=text/css href=https://xvnpw.github.io/css/my.6e08ae20ebbcf10b8953bc0c3935a825ae172c99096f9af1bf2df91a6d21a1be.css></head><body><div class=content><header><div class=main><a href=https://xvnpw.github.io/>xvnpw personal blog</a></div><nav><a href=/>Home</a>
<a href=/posts>All posts</a>
<a href=/about>About</a>
<a href=/tags>Tags</a></nav></header><main><article><div class=post-container><div class=post-content><div class=title><h1 class=title>AI Security Analyzer - All-in-One Tool Preview</h1><div class=meta>Posted on Dec 19, 2024</div></div><section class=body><p><a href=https://github.com/xvnpw/ai-security-analyzer>AI Security Analyzer</a> is my latest project - a powerful tool that leverages AI to automatically generate comprehensive security documentation for your projects. Whether you&rsquo;re dealing with security design, threat modeling, attack surface analysis, or more, this tool aims to simplify and enhance your security documentation process.</p><figure class=image-center><img src=https://github.com/user-attachments/assets/86ebd729-24ef-48cf-b704-3f42a8e34162 width=200></figure><p><strong>Watch the demo:</strong><figure class=image-center><img src=https://github.com/user-attachments/assets/a9de6ce7-9702-4fae-97a4-424d03a683eb></figure></p><h2 id=threats-vs-vulnerabilities>Threats vs. Vulnerabilities</h2><p>Before we dive in, I want to clarify an important distinction: <strong>threats</strong> vs. <strong>vulnerabilities</strong>. My tool focuses on identifying threats, which are <strong>potential risks</strong> that could be exploited by an attacker. Vulnerabilities, on the other hand, are specific weaknesses in a system that can be exploited. If you&rsquo;re looking for tools to identify vulnerabilities, I recommend checking out <a href=https://googleprojectzero.blogspot.com/2024/10/from-naptime-to-big-sleep.html>Google Project Zero</a>.</p><h2 id=background-and-motivation>Background and Motivation</h2><p>This isn&rsquo;t my first venture into using AI for generating security documentation. I&rsquo;ve previously developed the <a href=https://github.com/xvnpw/ai-threat-modeling-action>AI Threat Modeling Action</a> and contributed to the <a href=https://github.com/danielmiessler/fabric/blob/main/patterns/create_stride_threat_model/system.md>Threat Modeling Fabric Pattern</a>.</p><p>While both projects have been valuable, they came with limitations in flexibility and ease of use. That&rsquo;s why I decided to create the AI Security Analyzer. By leveraging <a href=https://academy.langchain.com/courses/intro-to-langgraph>LangGraph</a>, I can create more complex workflows and generate comprehensive, tailored security documents.</p><h2 id=5-different-types-of-documents>5 Different Types of Documents</h2><p>Application can generate 5 different types of documents:</p><ul><li>üîí Security Design Documentation - Generate security design documentation for your project.</li><li>üéØ Threat Modeling - Perform threat modeling for your project using STRIDE method for application, deployment and build.</li><li>üîç Attack Surface Analysis - Analyze the attack surface of your project to identify potential entry points.</li><li>‚ö†Ô∏è Threat Scenarios - Perform threat scenarios analysis for your project using <a href=https://danielmiessler.com/>Daniel Miessler&rsquo;s</a> <a href=https://github.com/danielmiessler/fabric/blob/f5f50cc4c94a539ee56bc533e9b1194eb9aa424d/patterns/create_threat_scenarios/system.md>prompt</a></li><li>üå≥ Attack Tree Analysis - Create attack trees to visualize potential attack vectors and their hierarchies.</li></ul><p>You can find the specific prompts for each document type in the <a href=https://github.com/xvnpw/ai-security-analyzer/blob/main/ai_security_analyzer/prompts.py#L92>prompts.py</a> file.</p><h2 id=3-different-ways-to-generate-documents>3 Different Ways to Generate Documents</h2><ul><li><strong><code>dir</code> Mode</strong>: Analyze a local directory by sending all (or filtered) files to the LLM. Ideal for existing projects where you want a comprehensive analysis.</li><li><strong><code>github</code> Mode</strong>: Analyze a GitHub repository using the LLM&rsquo;s knowledge base. This is effective for public repositories and models trained on GitHub data.</li><li><strong><code>file</code> Mode</strong>: Analyze a single file. This mode is similar to my previous projects and is useful for focused analysis.</li></ul><h2 id=practical-use-cases>Practical Use Cases</h2><ol><li><strong>Early Stage Security Review</strong>: Quick security assessment for new projects or codebases.</li><li><strong>Documentation Generation</strong>: Automate creation of security design docs for compliance requirements.</li><li><strong>Security Knowledge Base</strong>: Generate baseline security documentation that can be refined by security experts.</li><li><strong>Continuous Security Assessment</strong>: Integrate into CI/CD pipeline for ongoing security documentation updates.</li></ol><h2 id=how-to-use-the-ai-security-analyzer>How to Use the AI Security Analyzer</h2><p>Before getting started, make sure you&rsquo;ve checked the <a href=https://github.com/xvnpw/ai-security-analyzer#prerequisites>prerequisites and installation instructions</a>. Now, let&rsquo;s walk through an example using <code>dir</code> mode to analyze another one of my projects, <a href=https://github.com/xvnpw/fabric-agent-action>fabric-agent-action</a>.</p><h3 id=example-command>Example Command</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/xvnpw/fabric-agent-action <span style=color:#75715e># Clone the fabric-agent-action repository</span>
</span></span><span style=display:flex><span>cd ai-security-analyzer <span style=color:#75715e># Navigate to the ai-security-analyzer directory</span>
</span></span><span style=display:flex><span>poetry run python ai_security_analyzer/app.py dir -v -t /path/to/fabric-agent-action/ --dry-run --exclude <span style=color:#e6db74>&#34;**/prompts/**&#34;</span> -o FABRIC-AGENT-ACTION-o1-preview.md --agent-model o1-preview --agent-temperature <span style=color:#ae81ff>1</span> --agent-prompt-type sec-design
</span></span></code></pre></div><h4 id=breaking-down-the-command>Breaking Down the Command:</h4><ul><li><strong><code>dir</code></strong>: Specifies that we want to analyze a directory.</li><li><strong><code>-v</code></strong>: Enables verbose mode for detailed logging.</li><li><strong><code>-t /path/to/fabric-agent-action/</code></strong>: Sets the target directory for analysis.</li><li><strong><code>--dry-run</code></strong>: Performs a dry run to show what would happen without making API calls.</li><li><strong><code>--exclude "**/prompts/**"</code></strong>: Excludes the <code>prompts</code> directory using a glob pattern.</li><li><strong><code>-o FABRIC-AGENT-ACTION-o1-preview.md</code></strong>: Specifies the output file.</li><li><strong><code>--agent-model o1-preview</code></strong>: Sets the LLM model to <code>o1-preview</code>.</li><li><strong><code>--agent-temperature 1</code></strong>: Sets the temperature for the LLM. For <code>o1-preview</code>, only a temperature of 1 is accepted.</li><li><strong><code>--agent-prompt-type sec-design</code></strong>: Specifies that we want to generate a security design document.</li></ul><h3 id=understanding-the-output>Understanding the Output</h3><p>In dry run mode, you&rsquo;ll see output similar to this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>2024-12-19 11:11:44,844 - __main__ - INFO - Starting AI Security Analyzer
</span></span><span style=display:flex><span>2024-12-19 11:11:45,990 - ai_security_analyzer.full_dir_scan - INFO - Loading files
</span></span><span style=display:flex><span>2024-12-19 11:11:51,205 - ai_security_analyzer.full_dir_scan - INFO - Loaded <span style=color:#ae81ff>27</span> documents
</span></span><span style=display:flex><span>2024-12-19 11:11:51,209 - ai_security_analyzer.full_dir_scan - INFO - Sorting and filtering documents
</span></span><span style=display:flex><span>2024-12-19 11:11:51,209 - ai_security_analyzer.full_dir_scan - INFO - Documents after sorting and filtering: <span style=color:#ae81ff>27</span>
</span></span><span style=display:flex><span>2024-12-19 11:11:51,212 - ai_security_analyzer.full_dir_scan - INFO - Splitting documents
</span></span><span style=display:flex><span>2024-12-19 11:11:51,233 - ai_security_analyzer.full_dir_scan - INFO - Splitted documents into smaller chunks: 25
</span></span><span style=display:flex><span><span style=color:#f92672>===========</span> dry-run <span style=color:#f92672>===========</span>
</span></span><span style=display:flex><span>All documents token count: <span style=color:#ae81ff>33325</span>
</span></span><span style=display:flex><span>List of chunked files to analyze:
</span></span><span style=display:flex><span>..<span style=color:#ae81ff>\f</span>abric-agent-action<span style=color:#ae81ff>\R</span>EADME.md
</span></span><span style=display:flex><span>..<span style=color:#ae81ff>\f</span>abric-agent-action<span style=color:#ae81ff>\d</span>ocs<span style=color:#ae81ff>\u</span>pdating-fabric-patterns.md
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>2024-12-19 11:11:51,267 - __main__ - INFO - AI Security Analyzer completed successfully
</span></span></code></pre></div><p>This output tells you:</p><ul><li>The total number of documents and tokens to be processed.</li><li>The list of files that will be analyzed.</li><li>No API calls are made during a dry run, which helps you estimate the potential cost.</li></ul><h2 id=prompts-and-models-behavior>Prompts and Models Behavior</h2><p>A few observations about using prompts with LLMs:</p><ul><li>Prompts that have detailed instructions worked better than prompts that were more open-ended, e.g. &ldquo;create STRIDE threat model&rdquo; will sometimes make magic happen, but will not give repeatable results.</li><li>Models are bad at generating mermaid diagrams, so I&rsquo;ve implemented fixer - it will parse mermaid using javascript library and send error to LLM to fix it. Sadly this in not working 100% of the time. Models cannot get that &ldquo;(&rdquo; inside &ldquo;[ ]&rdquo; are not allowed ü§∑</li><li>Attack surface prompt still needs some work, but it does one thing extremely well - it provides best threat ranking from other prompts.</li><li>Security design prompt is sometimes giving marvels in risk assessment. It can very well describe relation between business and security risks.</li></ul><h2 id=highlights-and-examples>Highlights and Examples</h2><p>Here are some standout outputs generated by the AI Security Analyzer:</p><h3 id=1-security-design-documentation>1. Security Design Documentation</h3><p>For <a href=https://github.com/xvnpw/fabric-agent-action>fabric-agent-action</a>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span>Recommended security controls (high priority to implement if not already mentioned):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Validate the output from LLM before posting any publicly visible content.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## RISK ASSESSMENT
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>1.</span> Critical business processes we are trying to protect:
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> Automation around GitHub workflows that post or merge code, open pull requests, or alter repository content.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>2.</span> Data we are trying to protect and their sensitivity:
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> GitHub repository content (proprietary code).
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> LLM API tokens stored in GitHub Secrets to prevent uncontrolled usage or cost infiltration.
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> AI-generated outputs that could contain privileged or sensitive data if improperly handled by the LLM.
</span></span></code></pre></div><p>Model really nailed it with risk assessment. It&rsquo;s not just some random text, it&rsquo;s actually risk assessment that can be used to make decisions.</p><h3 id=2-threat-modeling>2. Threat Modeling</h3><p>For <a href=https://github.com/abi/screenshot-to-code>screenshot-to-code</a>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span><span style=color:#66d9ef>-</span> THREAT ID - 0001
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> COMPONENT NAME - Backend Application
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> THREAT NAME - Code Injection via Malicious Image Data
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> STRIDE CATEGORY - Tampering
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> WHY APPLICABLE - User-provided images are processed by the backend and sent to AI APIs. Malicious images could exploit vulnerabilities in image processing libraries or lead to code injection.
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> HOW MITIGATED - No mention of sanitization or input validation in the project files.
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> MITIGATION - Implement strict input validation and sanitization of user-provided images. Use secure libraries for image processing. Restrict allowed file types and enforce file size limits.
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> LIKELIHOOD EXPLANATION - Medium likelihood as attackers may attempt to exploit image parsing vulnerabilities by uploading crafted images.
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> IMPACT EXPLANATION - High impact as successful exploitation could allow execution of arbitrary code on the backend server, compromising the entire system and potentially exposing sensitive data and API keys.
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> RISK SEVERITY - High
</span></span></code></pre></div><p>This is a high-quality threat analysis that aligns with what one might produce during manual threat modeling. LIKELIHOOD and IMPACT EXPLANATION are my usual way of pushing models to give better results on RISK SEVERITY. Not sure if it&rsquo;s working, but it&rsquo;s worth a try.</p><h3 id=3-attack-tree-analysis>3. Attack Tree Analysis</h3><p>For <a href=https://github.com/pallets/flask>Flask</a>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span><span style=color:#75715e>## 7. Analyze and Prioritize Attack Paths
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#75715e>### High-Risk Paths
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=font-weight:700>**2. Exploit Template Rendering Vulnerabilities**</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> **Justification**: Template rendering vulnerabilities, specifically Server-Side Template Injection (SSTI), can allow attackers to execute arbitrary code on the server. Given that developers might inadvertently use <span style=color:#e6db74>`render_template_string`</span> with user input or disable autoescaping, this poses a critical risk.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>**5. Exploit Insecure Configurations in Flask Applications**</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> **Justification**: Misconfigurations, such as leaving DEBUG mode enabled or missing security headers, are common and can have severe consequences. Attackers can easily exploit these to gain sensitive information or bypass security protections.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>**7. Exploit Cross-Site Scripting (XSS) Vulnerabilities**</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> **Justification**: XSS vulnerabilities are prevalent in web applications. If developers improperly handle user input or disable autoescaping, attackers can inject malicious scripts, leading to data theft or session hijacking.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>**8. Exploit Cross-Site Request Forgery (CSRF) Vulnerabilities**</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> **Justification**: Without proper CSRF protection, attackers can trick authenticated users into performing unwanted actions. Given that CSRF tokens are not automatically implemented in Flask, developers may overlook this protection.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>**10. Exploit Misconfiguration of Security Headers**</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> **Justification**: Missing security headers like CSP and HSTS can expose applications to XSS attacks and downgrade attacks. Since Flask does not set these headers by default, developers need to proactively implement them.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>### Critical Nodes
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> **5.2 DEBUG Mode Enabled in Production**: Ensuring DEBUG mode is disabled in production environments is crucial to prevent information disclosure.
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> **7.1 Developer Renders User Input Without Proper Escaping**: Properly escaping user input prevents XSS attacks.
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> **8.1 Missing CSRF Protection in Forms**: Implementing CSRF tokens in forms is essential to prevent CSRF attacks.
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> **10.1 Missing Content Security Policy (CSP) Header**: Setting a CSP header mitigates XSS risks.
</span></span></code></pre></div><p>While some points may seem generic, the model effectively identified critical misconfigurations that are common pitfalls in Flask applications.</p><h2 id=conclusion>Conclusion</h2><p>I&rsquo;m thrilled with how the AI Security Analyzer has turned out. It addresses many of the challenges I&rsquo;ve encountered since working with GPT-3.5 and offers a robust solution for generating security documentation.</p><p>Some might wonder how and why someone would use this tool. I believe it&rsquo;s an excellent starting point for understanding the security posture of your project. It helps navigate through complexities and provides a foundation that you can build upon.</p><p>Let me know what you think about it! I&rsquo;m waiting for your feedback. Feel free to reach out!</p><p><a href=https://github.com/xvnpw/ai-security-analyzer>Code</a> and <a href=https://github.com/xvnpw/ai-security-analyzer/tree/main/examples>examples</a> are available on GitHub.</p><hr><p>Thanks for reading! You can contact me and/or follow me on <a href=https://x.com/xvnpw>X</a> and <a href=www.linkedin.com/in/marcin-niemiec-304349104>LinkedIn</a>.</p></section><div class=post-tags><nav class="nav tags"><ul class=tags><li><a href=/tags/security>security</a></li><li><a href=/tags/llm>llm</a></li><li><a href=/tags/gpt>gpt</a></li><li><a href=/tags/ai>ai</a></li><li><a href=/tags/github>github</a></li><li><a href=/tags/langchain>langchain</a></li></ul></nav></div></div></div></article></main><footer><div style=display:flex><a class=soc href=https://github.com/xvnpw rel=me title=GitHub><svg class="feather"><use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#github"/></svg></a><a class=border></a><a class=soc href=https://twitter.com/xvnpw rel=me title=Twitter><svg class="feather"><use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#twitter"/></svg></a><a class=border></a><a class=soc href=https://www.linkedin.com/in/marcin-niemiec-304349104/ rel=me title=Linkedin><svg class="feather"><use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#linkedin"/></svg></a><a class=border></a></div><div class=footer-info>2025 <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a></div></footer><script async src="https://www.googletagmanager.com/gtag/js?id=G-YHETMXZXMZ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-YHETMXZXMZ")}</script></div></body></html>