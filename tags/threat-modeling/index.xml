<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Threat-Modeling on xvnpw personal blog</title><link>https://xvnpw.github.io/tags/threat-modeling/</link><description>Recent content in Threat-Modeling on xvnpw personal blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 02 Feb 2025 09:00:00 +0100</lastBuildDate><atom:link href="https://xvnpw.github.io/tags/threat-modeling/index.xml" rel="self" type="application/rss+xml"/><item><title>Forget Threats, Mitigations are All You REALLY Need</title><link>https://xvnpw.github.io/posts/forget-threats-mitigations-are-all-you-really-need/</link><pubDate>Sun, 02 Feb 2025 09:00:00 +0100</pubDate><guid>https://xvnpw.github.io/posts/forget-threats-mitigations-are-all-you-really-need/</guid><description>&lt;p>Ever feel like you&amp;rsquo;re speaking a different language when you talk to developers about security? You&amp;rsquo;re buzzing with threat models, attack vectors, and vulnerabilities, while they&amp;rsquo;re focused on features, deadlines, and, well, &lt;em>making things work&lt;/em>. I get it. I&amp;rsquo;ve been there. And recently, I had a bit of an &amp;ldquo;aha!&amp;rdquo; moment that completely shifted my perspective on security discussions.&lt;/p>
&lt;p>It happened during a review of security analysis with my colleagues at Form3. We were diving deep into the &lt;a href="https://github.com/form3tech-oss/terraform-provider-chronicle">terraform-provider-chronicle&lt;/a> project (you can see analysis &lt;a href="https://github.com/xvnpw/ai-security-analyzer/tree/main/examples/form3tech-oss/README.md">here&lt;/a>, using my &lt;a href="https://github.com/xvnpw/ai-security-analyzer">ai-security-analyzer&lt;/a> tool). As we talked through AI generated threats, it struck me: to make AI tools useful for developers, we need to focus on mitigations, not threats. Developers are less interested in abstract threats and far more engaged when you talk about &lt;strong>mitigations&lt;/strong> â€“ concrete steps to &lt;em>fix&lt;/em> things.&lt;/p></description></item><item><title>Deep Analysis Mode in AI Security Analyzer</title><link>https://xvnpw.github.io/posts/ai-security-analyzer-deep-analysis-mode/</link><pubDate>Fri, 10 Jan 2025 20:00:00 +0100</pubDate><guid>https://xvnpw.github.io/posts/ai-security-analyzer-deep-analysis-mode/</guid><description>&lt;p>First off, &lt;strong>a big thank you&lt;/strong> ðŸ™‡ to everyone who provided such positive feedback on my previous post, &lt;a href="https://xvnpw.github.io/posts/scaling-threat-modeling-with-ai/">Scaling Threat Modeling with AI: Generating 1000 Threat Models Using Gemini 2.0 and AI Security Analyzer&lt;/a>. Your insights and suggestions have been incredibly valuable.&lt;/p>
&lt;p>Inspired by your comments, I&amp;rsquo;ve added a new feature to the AI Security Analyzer: &lt;strong>Deep Analysis Mode&lt;/strong>. In this post, I&amp;rsquo;ll walk you through how it works and showcase its capabilities using Google&amp;rsquo;s Gemini 2.0 Flash Thinking Experimental model to perform an in-depth threat modeling on the &lt;a href="https://github.com/pallets/flask">Flask&lt;/a> project. We&amp;rsquo;ll compare outputs between Normal Mode and Deep Analysis Mode to highlight the differences.&lt;/p></description></item><item><title>Threat Modelling with Fabric Framework</title><link>https://xvnpw.github.io/posts/threat_modelling_with_fabric_framework/</link><pubDate>Mon, 03 Jun 2024 16:59:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/threat_modelling_with_fabric_framework/</guid><description>&lt;p>&lt;a href="https://github.com/danielmiessler/fabric">Fabric&lt;/a> is a framework that puts AI at your fingertips. Instead of diving into chat interfaces (e.g., ChatGPT) or writing custom programs that consume APIs, you can create prompts as markdown text and receive output in markdown. Fabric also maintains a database of prompts called &lt;a href="https://github.com/danielmiessler/fabric/tree/main/patterns">patterns&lt;/a>.&lt;/p>
&lt;figure class="image-center">&lt;img src="https://github.com/xvnpw/xvnpw.github.io/assets/17719543/4fe91d36-3736-4cbf-9835-edfa3943116e" width="300">
&lt;/figure>

&lt;p>With the new pattern &lt;a href="https://github.com/danielmiessler/fabric/blob/main/patterns/create_stride_threat_model/system.md">create_stride_threat_model&lt;/a>, you can easily create threat models. Let&amp;rsquo;s dive deeper into how to use this new pattern and evaluate the quality of the results.&lt;/p></description></item><item><title>Leveraging LLMs for Threat Modeling - Claude 3 Opus vs GPT-4</title><link>https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-claude-3-vs-gpt-4/</link><pubDate>Wed, 20 Mar 2024 14:59:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-claude-3-vs-gpt-4/</guid><description>&lt;p>&lt;a href="https://www.anthropic.com/news/claude-3-family">Claude 3 Opus&lt;/a> is the latest and most powerful model from Anthropic. Is it able to overcome GPT-4?&lt;/p>
&lt;figure class="image-center">&lt;img src="https://github.com/xvnpw/xvnpw.github.io/assets/17719543/3305cef0-c07d-4239-8fd9-c6e9e14146e7">
&lt;/figure>

&lt;h2 id="revisiting-the-experiment">Revisiting the Experiment&lt;/h2>
&lt;p>If you wish to understand more about the experiment structure, you can refer to my &lt;a href="https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5/">previous post&lt;/a>. But here&amp;rsquo;s a quick recap:&lt;/p>
&lt;p>I used markdown files describing a fictional project, &lt;strong>AI Nutrition-Pro&lt;/strong>, with input:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/xvnpw/ai-nutrition-pro-design-claude3-opus/blob/main/PROJECT.md">PROJECT.md&lt;/a> - high level project description&lt;/li>
&lt;li>&lt;a href="https://github.com/xvnpw/ai-nutrition-pro-design-claude3-opus/blob/main/ARCHITECTURE.md">ARCHITECTURE.md&lt;/a> - architecture description&lt;/li>
&lt;li>&lt;a href="https://github.com/xvnpw/ai-nutrition-pro-design-claude3-opus/blob/main/user-stories/0001_STORE_DIET_INTRODUCTIONS.md">0001_STORE_DIET_INTRODUCTIONS.md&lt;/a> - user story&lt;/li>
&lt;/ul>
&lt;p>I tasked the AI models with four types of analysis: high-level security design review, threat modeling, security-related acceptance criteria and review of architecture:&lt;/p></description></item><item><title>Reviewing Your Architecture Using LLMs</title><link>https://xvnpw.github.io/posts/review_your_architecture_using_llms/</link><pubDate>Wed, 25 Oct 2023 08:59:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/review_your_architecture_using_llms/</guid><description>&lt;p>Recently, I&amp;rsquo;ve discussed &lt;a href="https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5-vs-claude2-vs-gpt-4/">utilizing LLMs for threat modeling&lt;/a>. Building on that, I&amp;rsquo;ve incorporated a new feature in my &lt;a href="https://github.com/xvnpw/ai-threat-modeling-action">ai-threat-modeling-action&lt;/a> on GitHub. It now allows you to review your project&amp;rsquo;s architectural description using LLMs. Let&amp;rsquo;s delve deeper ðŸ¤¿!&lt;/p>
&lt;h2 id="revisiting-the-experiment">Revisiting the Experiment&lt;/h2>
&lt;p>For a detailed understanding of the experiment&amp;rsquo;s structure, you can revisit my &lt;a href="https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5/">previous post&lt;/a>. However, here&amp;rsquo;s a brief summary:&lt;/p>
&lt;p>I utilized markdown files, which described a fictional project named &lt;strong>AI Nutrition-Pro&lt;/strong>, as input:&lt;/p></description></item><item><title>Leveraging LLMs for Threat Modeling - GPT-3.5 vs Claude 2 vs GPT-4</title><link>https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5-vs-claude2-vs-gpt-4/</link><pubDate>Sun, 03 Sep 2023 08:59:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5-vs-claude2-vs-gpt-4/</guid><description>&lt;p>In a bid to uncover which AI model is best at threat modeling, I put GPT-3.5, Claude 2, and GPT-4 to the test. Let&amp;rsquo;s see how they performed.&lt;/p>
&lt;figure class="image-center">&lt;img src="https://github.com/xvnpw/xvnpw.github.io/assets/17719543/bf036c66-1d66-4468-8dcd-426d6e0f40f6">
&lt;/figure>

&lt;p>&lt;a href="https://twitter.com/emollick">Ethan Mollick&lt;/a>, a professor at The Wharton School, once said something that perfectly captures my experience with these AI models. It feels as if they&amp;rsquo;re striving to answer questions in the simplest way possible ðŸ˜ However, you can get around this by asking for detailed explanations or step-by-step thinking.&lt;/p></description></item><item><title>Leveraging LLMs for Threat Modeling - GPT-3.5</title><link>https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5/</link><pubDate>Thu, 17 Aug 2023 18:59:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5/</guid><description>&lt;p>In this article, I delve into the &lt;a href="https://github.com/xvnpw/ai-nutrition-pro-design-gpt3.5">AI Nutrition-Pro experiment&lt;/a>, a research project exploring the potential of LLMs in enhancing security practices during the design phase of DevSecOps: &lt;strong>threat modeling&lt;/strong> and &lt;strong>security review&lt;/strong>.&lt;/p>
&lt;h2 id="devsecops-a-brief-overview">DevSecOps: A Brief Overview&lt;/h2>
&lt;p>DevSecOps merges the principles of development, security, and operations to create a culture of shared responsibility for software security. The three main goals of DevSecOps are:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Shift Left Security:&lt;/strong> Identifying and addressing security vulnerabilities as early as possible in the software development lifecycle.&lt;/li>
&lt;li>&lt;strong>Developer-Centric:&lt;/strong> Integrating security practices seamlessly into the developer&amp;rsquo;s ecosystem, including Integrated Development Environments (IDEs), code hosting platforms, and pull requests.&lt;/li>
&lt;li>&lt;strong>Fast Feedback and Guidance:&lt;/strong> Providing developers with rapid feedback on security issues and guidance on secure coding practices.&lt;/li>
&lt;/ul>
&lt;p>While security tools like &lt;a href="https://semgrep.dev/blog/2023/using-ai-to-write-secure-code-with-semgrep">semgrep&lt;/a> can already use LLMs in the coding phase, the AI Nutrition-Pro experiment seeks to explore the benefits of LLMs during the design phase, particularly in security design reviews and threat modeling.&lt;/p></description></item><item><title>Threat Modeling 101</title><link>https://xvnpw.github.io/posts/threat_modeling_101/</link><pubDate>Wed, 19 Oct 2022 18:59:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/threat_modeling_101/</guid><description>&lt;p>What is Threat Modeling? First of all, it&amp;rsquo;s just thinking about threats. We all do it, every day ðŸ˜ƒ &amp;ldquo;How someone could break into my house?&amp;rdquo; But wait a second. How do you know that you need to protect your house in the first place? Maybe you don&amp;rsquo;t have a house, or maybe you don&amp;rsquo;t have money right now to buy deterrents. Or maybe your family thinks you are a bit paranoid? ðŸ˜• Just before doing Threat Modeling you need to start doing Risk Management, to assess what is your risk appetite and profile. More on that later.&lt;/p></description></item></channel></rss>