---
title: "Sonnet 3.7 and New Vulnerability Workflow"
date: 2025-03-01T16:00:00+01:00
draft: true
tags: [security, ai, vulnerabilities, screenshot-to-code, github, llm]
description: "Showcasing a new vulnerability workflow using Sonnet 3.7 and a custom prompt."
---

With release new model from Anthropic, Sonnet 3.7, I've updated my [ai-security-analyzer](https://github.com/xvnpw/ai-security-analyzer) to use the new model. I also tested it as same benchmark as other models in my previous [post]({{< ref "posts/can-ai-actually-find-real-security-bugs-testing-the-new-wave-of-ai-models.md" >}}).

## How to use Sonnet 3.7

To use Sonnet 3.7, you can run the following command:


```bash
python ai_security_analyzer/app.py \
    dir \
    -t ../path/to/project \
    -o vulnerabilities.md \
    --agent-prompt-type vulnerabilities \
    --agent-provider openrouter \ # or anthropic
    --agent-model anthropic/claude-3.7-sonnet:thinking \ # or claude-3.7-sonnet-latest
    --agent-temperature 1
```

## The Experiment: Testing Against Real Code

### Methodology

I'll run the [prompt](https://github.com/xvnpw/ai-security-analyzer/blob/main/ai_security_analyzer/prompts/default/dir/vulnerabilities-default.yaml) against a project I'm familiar with: [screenshot-to-code](https://github.com/abi/screenshot-to-code). I'll then manually verify each reported vulnerability by attempting to reproduce it. This will give us a sense of the true positive and false positive rates model.

## Results and Analysis

Here's a summary of the results for each model. I've included detailed outputs in the `ai-security-analyzer` repository.

### Anthropic Sonnet 3.7 Thinking

Detailed output: [vulnerabilities.md](https://github.com/xvnpw/ai-security-analyzer/blob/main/examples/dir-vulnerabilities-screenshot-to-code-anthropicclaude-3.7-sonnetthinking.md)

| Vulnerability | Valid | Comment |
| --- | --- | --- |

...

## Vulnerability Workflow 1

While getting mix results from this and other models, I started to think about what is the best way to use AI to find vulnerabilities in code. As, I was not ready to implement something so complex as Google's [Project Zero](https://googleprojectzero.blogspot.com/2024/10/from-naptime-to-big-sleep.html), I decided to test a simple workflow.

### Graph

Workflow can be described with the following graph:

{{< figure src="https://github.com/user-attachments/assets/00af6613-c215-4a39-a6a8-bc3387432bc4"  class="image-center" >}}

Idea is to:
- use two models to find vulnerabilities: primary and secondary,
- use same prompt as for __normal__ vulnerability analysis,
- add filter step to remove not relevant vulnerabilities,
- add merge step to get final list of vulnerabilities,

What is the benefit?
- having two different models, should give better results,
- what is more interesting, I can use those two models multiple times, getting two, three, four, etc. results for each model,

### Reliability

Graph looks promising, but there is catch. For each step involving LLM, we can define reliability of the result. No in technical terms, but in terms of confidence and accuracy of the result. Let's say for give step **s**, we can get 70 out of 100 results as valid. It means that we have 0.7 accuracy for this step. Each step, and each model, can have different accuracy.

Give some numbers, we can calculate accuracy of the whole workflow. All accuracy numbers are **made up**. It's just for example:

| Step | Model | Accuracy | Description |
| --- | --- | --- | --- |
| Primary Vulnerability Analysis | Sonnet 3.7 Thinking | 0.392 | It's accuracy of whole task. It contains multiple steps. I will explain it later. |
| Secondary Vulnerability Analysis | OpenAI o3 mini | 0.576 | Same as above. It's just a number to show that it's not the same as primary. |
| Filter | Sonnet 3.7 Thinking | 0.8 | It's simple task, so it's should be reliable |
| Merge | Sonnet 3.7 Thinking | 0.8 | It's simple task, so it's should be reliable |

```
---> Sonnet 3.7 Thinking --> 0.392 * 0.8 = 0.3136
---> OpenAI o3 mini --> 0.576 * 0.8 = 0.4608

We have two branches:
--> 1 - (1 - 0.3136) * (1 - 0.4608) = 0.6298

And final step is to merge two branches:
--> 0.6298 * 0.8 = 0.5038
```

Key observations:
- Filter step is lowering accuracy of the whole branch. 
- Final accuracy can be lower then accuracy of individual vulnerability analysis.
- Choosing right model for each task is important.

How to improve accuracy?

Let's try we would like to have accuracy >= 70%.

| Number of iterations | Accuracy | Description |
| --- | --- | --- |
| 2 | 0.5038 | Two iterations - one with primary and one with secondary models |
| 3 | 0.5967 | Three iterations - two with primary, one with secondary models |
| 4 | 0.6904 | Four iterations - two with primary, two with secondary models |
| 5 | 0.8 | Five iterations - three with primary, two with secondary models |

In such case, we need to have 5 iterations to get accuracy >= 70%.

#### Vulnerability Analysis Reliability

Going back to Primary and Secondary Vulnerability Analysis steps. Each step contains multiple steps. For example, for repository with 3 batches of files:

- intermediate step 1: 0.8 accuracy (it's step without existing document)
- intermediate step 2: 0.7 accuracy (it's step with existing document to update with new findings from new batch)
- intermediate step 3: 0.7 accuracy (it's step with existing document to update with new findings from new batch)

In such case, accuracy of vulnerability analysis is: 0.8 * 0.7 * 0.7 = 0.392. It means that we have 39.2% accuracy of finding vulnerabilities.

Those numbers are made up as well. It's just for example.
















