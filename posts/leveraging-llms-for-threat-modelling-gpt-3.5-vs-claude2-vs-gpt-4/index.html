<!doctype html><html><head lang=en><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Leveraging LLMs for Threat Modeling - GPT-3.5 vs Claude 2 vs GPT-4 - xvnpw personal blog</title><link rel=icon type=image/png href=/favicon.png><meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="We put the leading AI models to the test in threat modeling. Let's dive into the results and see which one comes out on top."><meta property="og:image" content><meta property="og:url" content="https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5-vs-claude2-vs-gpt-4/"><meta property="og:site_name" content="xvnpw personal blog"><meta property="og:title" content="Leveraging LLMs for Threat Modeling - GPT-3.5 vs Claude 2 vs GPT-4"><meta property="og:description" content="We put the leading AI models to the test in threat modeling. Let's dive into the results and see which one comes out on top."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-09-03T08:59:02+01:00"><meta property="article:modified_time" content="2023-09-03T08:59:02+01:00"><meta property="article:tag" content="Security"><meta property="article:tag" content="Threat-Modeling"><meta property="article:tag" content="Langchain"><meta property="article:tag" content="Llm"><meta property="article:tag" content="Gpt"><meta property="article:tag" content="Claude"><meta name=twitter:card content="summary"><meta name=twitter:title content="Leveraging LLMs for Threat Modeling - GPT-3.5 vs Claude 2 vs GPT-4"><meta name=twitter:description content="We put the leading AI models to the test in threat modeling. Let's dive into the results and see which one comes out on top."><script src=https://xvnpw.github.io/js/feather.min.js></script><link href=https://xvnpw.github.io/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://xvnpw.github.io/css/main.5cebd7d4fb2b97856af8d32a6def16164fcf7d844e98e236fcb3559655020373.css><link id=darkModeStyle rel=stylesheet type=text/css href=https://xvnpw.github.io/css/dark.d22e2a2879d933a4b781535fc4c4c716e9f9d35ea4986dd0cbabda82effc4bdd.css disabled><link rel=stylesheet type=text/css href=https://xvnpw.github.io/css/my.0b83e2a92adf566deab02a012390927b24d79d5dc5a21a839eb61419dc041acc.css></head><body><div class=content><header><div class=main><a href=https://xvnpw.github.io/>xvnpw personal blog</a></div><nav><a href=/>Home</a>
<a href=/posts>All posts</a>
<a href=/about>About</a>
<a href=/tags>Tags</a>
| <span id=dark-mode-toggle onclick=toggleTheme()></span>
<script src=https://xvnpw.github.io/js/themetoggle.js></script></nav></header><main><article><div class=title><h1 class=title>Leveraging LLMs for Threat Modeling - GPT-3.5 vs Claude 2 vs GPT-4</h1><div class=meta>Posted on Sep 3, 2023</div></div><section class=body><p>In a bid to uncover which AI model is best at threat modeling, I put GPT-3.5, Claude 2, and GPT-4 to the test. Let&rsquo;s see how they performed.</p><figure class=image-center><img src=https://github.com/xvnpw/xvnpw.github.io/assets/17719543/bf036c66-1d66-4468-8dcd-426d6e0f40f6></figure><p><a href=https://twitter.com/emollick>Ethan Mollick</a>, a professor at The Wharton School, once said something that perfectly captures my experience with these AI models. It feels as if they&rsquo;re striving to answer questions in the simplest way possible üòè However, you can get around this by asking for detailed explanations or step-by-step thinking.</p><h2 id=revisiting-the-experiment>Revisiting the Experiment</h2><p>If you wish to understand more about the experiment structure, you can refer to my <a href=https://xvnpw.github.io/posts/leveraging-llms-for-threat-modelling-gpt-3.5/>previous post</a>. But here&rsquo;s a quick recap:</p><p>I used markdown files describing a fictional project, <strong>AI Nutrition-Pro</strong>, as input data:</p><ul><li><a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt3.5/blob/main/PROJECT.md>PROJECT.md</a> - high level project description</li><li><a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt3.5/blob/main/ARCHITECTURE.md>ARCHITECTURE.md</a> - architecture description</li><li><a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt3.5/blob/main/user-stories/0001_STORE_DIET_INTRODUCTIONS.md>0001_STORE_DIET_INTRODUCTIONS.md</a> - user story</li></ul><p>I tasked the AI models with three types of analysis: high-level security design review, threat modeling, and security-related acceptance criteria:</p><ul><li>High level security design review (example: <a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt3.5/blob/main/PROJECT_SECURITY.md>GPT-3.5</a>, <a href=https://github.com/xvnpw/ai-nutrition-pro-design-claude2/blob/main/PROJECT_SECURITY.md>Claude 2</a>, <a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt4/blob/main/PROJECT_SECURITY.md>GPT-4</a>)</li><li>Threat Modeling (example: <a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt3.5/blob/main/ARCHITECTURE_SECURITY.md>GPT-3.5</a>, <a href=https://github.com/xvnpw/ai-nutrition-pro-design-claude2/blob/main/ARCHITECTURE_SECURITY.md>Claude 2</a>, <a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt4/blob/main/ARCHITECTURE_SECURITY.md>GPT-4</a>)</li><li>Security related acceptance criteria (example: <a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt3.5/blob/main/user-stories/0001_STORE_DIET_INTRODUCTIONS_SECURITY.md>GPT-3.5</a>, <a href=https://github.com/xvnpw/ai-nutrition-pro-design-claude2/blob/main/user-stories/0001_STORE_DIET_INTRODUCTIONS_SECURITY.md>Claude 2</a>, <a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt4/blob/main/user-stories/0001_STORE_DIET_INTRODUCTIONS_SECURITY.md>GPT-4</a>)</li></ul><h2 id=key-learnings>Key Learnings</h2><h3 id=making-assumptions>Making Assumptions</h3><p>Threat modeling is both an art and a science, largely due to the role of assumptions. These assumptions guide the process and filter potential threats. In my research, I didn&rsquo;t set any assumptions to see how the AI models would handle it. They reacted much like a novice, offering a variety of different potential attacks. This can be managed by listing assumptions directly.</p><p>You can overcome that by listing assumptions directly:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-gdscript3 data-lang=gdscript3><span style=display:flex><span>Instruction:
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> I will provide you Assumptions that you need to consider <span style=color:#f92672>in</span> listing threats
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Assumptions:
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> SQL Injection threat is already mitigated using SAST <span style=color:#66d9ef>tool</span>
</span></span><span style=display:flex><span><span style=color:#f92672>...</span>
</span></span></code></pre></div><h3 id=seeking-explanations>Seeking Explanations</h3><p>Asking the AI models to explain why a threat applies to the architecture can yield good results, making the list of threats less random.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Instruction:
</span></span><span style=display:flex><span>- Explanation whether or not this threat is already mitigated in architecture
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>There&rsquo;s a noticeable difference in the understanding of explanations between GPT-3.5 and GPT-4:</p><p><strong>GPT-3.5</strong>:</p><ul><li><strong>Threat name:</strong> Attacker spoofs client identity and gains unauthorized access to API Gateway</li><li><strong>Explanation:</strong> This threat is applicable as API Gateway is the entry point for client requests and authentication is required to ensure the client&rsquo;s identity.</li><li><strong>Mitigations:</strong> Implement strong authentication mechanisms such as API keys, OAuth, or JWT tokens to verify the client&rsquo;s identity.</li></ul><p><strong>GPT-4</strong>:</p><ul><li><strong>Threat name:</strong> Attacker bypasses API Gateway and directly accesses API Application</li><li><strong>Explanation:</strong> This threat is applicable if the API Application is not properly secured and can be accessed without going through the API Gateway.</li><li><strong>Mitigations:</strong> Ensure that the API Application is only accessible through the API Gateway and cannot be directly accessed.</li></ul><p>It could be taken into higher level with step by step thinking. But didn&rsquo;t test that yet:</p><blockquote><p>When I give you something to do, you will convert that to a step by step plan and tell me what the step by step plan is.</p></blockquote><h3 id=understanding-components>Understanding Components</h3><p>The term &ldquo;component&rdquo; is often overloaded and can mean different things, which poses a problem for AI models, especially GPT-3.5 and Claude 2. After several tries, I found a way to clarify the meaning of &ldquo;component&rdquo; for them:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>Instruction:
</span></span><span style=display:flex><span>- List data flows that starts from internal or ends in internal architecture container that are important for security of system
</span></span><span style=display:flex><span>- Data flow should contain two items: A -&gt; B (A and B are architecture containers)
</span></span><span style=display:flex><span>- Architecture container is for example: service, microservice, database, queue, person, gateway, lambda, application
</span></span><span style=display:flex><span>- Architecture containers are included in C4 model in Container diagram
</span></span></code></pre></div><p>It&rsquo;s not perfect, because it expects architecture to be C4 model. This limits flexibility of prompt.</p><p>BTW. GPT-4 can understand what component is üòä</p><h3 id=moving-to-json>Moving to JSON</h3><p>Initially, I asked the AI models to format their output as markdown. However, I switched to JSON for added flexibility and ease of creating my own markdown. Both GPT models handled this well, though Claude 2 sometimes produced malformed documents. Solution was to use <a href=https://python.langchain.com/docs/modules/model_io/output_parsers/output_fixing_parser>OutputFixingParser</a> from <code>langchain</code>. If it detects malformed json it ask LLM to fix it üòè</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-fallback data-lang=fallback><span style=display:flex><span>fixing_parser = OutputFixingParser.from_llm(parser=parser, llm=llm)
</span></span><span style=display:flex><span>gen_threats = fixing_parser.parse(ret)
</span></span></code></pre></div><h2 id=the-verdict-gpt-35-vs-claude-2-vs-gpt-4>The Verdict: GPT-3.5 vs Claude 2 vs GPT-4</h2><p>GPT-3.5 and Claude 2 produced similar results. They&rsquo;re useful if your development team lacks extensive security experience, providing a reasonable number of important considerations. However, GPT-4 proved to be the superior model. It&rsquo;s less sensitive to changes in prompts and can be used to build robust threat modeling automation with the right assumptions.</p><p>üî• Check results yourself: <a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt3.5/blob/main/ARCHITECTURE_SECURITY.md>GPT-3.5</a>, <a href=https://github.com/xvnpw/ai-nutrition-pro-design-claude2/blob/main/ARCHITECTURE_SECURITY.md>Claude 2</a>, <a href=https://github.com/xvnpw/ai-nutrition-pro-design-gpt4/blob/main/ARCHITECTURE_SECURITY.md>GPT-4</a> and let me know what you think about experiment!</p><p><a href=https://github.com/xvnpw/ai-threat-modeling-action>Code</a> used in this experiment is published on github.</p><hr><p>Thanks for reading! You can contact me and/or follow on <a href=https://twitter.com/xvnpw>X/Twitter</a>.</p></section><div class=post-tags><nav class="nav tags"><ul class=tags><li><a href=/tags/security>security</a></li><li><a href=/tags/threat-modeling>threat-modeling</a></li><li><a href=/tags/langchain>langchain</a></li><li><a href=/tags/llm>llm</a></li><li><a href=/tags/gpt>gpt</a></li><li><a href=/tags/claude>claude</a></li></ul></nav></div></article></main><footer><div style=display:flex><a class=soc href=https://github.com/xvnpw rel=me title=GitHub><i data-feather=github></i></a>
<a class=border></a><a class=soc href=https://twitter.com/xvnpw rel=me title=Twitter><i data-feather=twitter></i></a>
<a class=border></a><a class=soc href=https://www.linkedin.com/in/marcin-niemiec-304349104/ rel=me title=Linkedin><i data-feather=linkedin></i></a>
<a class=border></a></div><div class=footer-info>2024 <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a></div></footer><script async src="https://www.googletagmanager.com/gtag/js?id=G-YHETMXZXMZ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-YHETMXZXMZ")}</script><script>feather.replace()</script></div></body></html>