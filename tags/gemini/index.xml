<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gemini on xvnpw personal blog</title><link>https://xvnpw.github.io/tags/gemini/</link><description>Recent content in Gemini on xvnpw personal blog</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Wed, 01 Jan 2025 12:19:02 +0100</lastBuildDate><atom:link href="https://xvnpw.github.io/tags/gemini/index.xml" rel="self" type="application/rss+xml"/><item><title>Scaling Threat Modeling with AI: Generating 1000 Threat Models Using Gemini 2.0 and AI Security Analyzer</title><link>https://xvnpw.github.io/posts/scaling-threat-modeling-with-ai/</link><pubDate>Wed, 01 Jan 2025 12:19:02 +0100</pubDate><guid>https://xvnpw.github.io/posts/scaling-threat-modeling-with-ai/</guid><description>&lt;p>&amp;ldquo;Can AI help us scale security analysis?&amp;rdquo; This question led me down a fascinating path of experimenting with Google&amp;rsquo;s Gemini 2.0 to generate threat models at an unprecedented scale. In this post, I&amp;rsquo;ll share how I turned this ambitious idea into reality, complete with code samples, real outputs, and valuable lessons learned along the way.&lt;/p>
&lt;h2 id="the-challenge-automating-security-analysis-at-scale">The Challenge: Automating Security Analysis at Scale&lt;/h2>
&lt;p>Security documentation is crucial but often becomes a bottleneck in fast-moving development cycles. With the release of Google&amp;rsquo;s Gemini 2.0 Flash Thinking Experimental model, I saw an opportunity to tackle this challenge head-on, despite some notable limitations:&lt;/p></description></item></channel></rss>