<!doctype html><html><head lang=en><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Threat Modeling with LLMs: Two Years In - Hype, Hope, and a Look at Gemini 2.5 Pro - xvnpw personal blog</title><link rel=icon type=image/png href=/favicon.png><meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="After two years of exploring AI for threat modeling, this post reviews the progress, tests Gemini 2.5 Pro, and reflects on the evolving potential and limitations of LLMs in cybersecurity."><meta property="og:image" content><meta property="og:url" content="https://xvnpw.github.io/posts/threat-modeling-with-llms-two-years-in-hype-hope-and-a-look-at-gemini-2.5-pro/"><meta property="og:site_name" content="xvnpw personal blog"><meta property="og:title" content="Threat Modeling with LLMs: Two Years In - Hype, Hope, and a Look at Gemini 2.5 Pro"><meta property="og:description" content="After two years of exploring AI for threat modeling, this post reviews the progress, tests Gemini 2.5 Pro, and reflects on the evolving potential and limitations of LLMs in cybersecurity."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-18T07:59:02+01:00"><meta property="article:modified_time" content="2025-05-18T07:59:02+01:00"><meta property="article:tag" content="Ai"><meta property="article:tag" content="Llm"><meta property="article:tag" content="Threat-Modeling"><meta property="article:tag" content="Cybersecurity"><meta property="article:tag" content="Chatgpt"><meta property="article:tag" content="Gemini"><meta name=twitter:card content="summary"><meta name=twitter:title content="Threat Modeling with LLMs: Two Years In - Hype, Hope, and a Look at Gemini 2.5 Pro"><meta name=twitter:description content="After two years of exploring AI for threat modeling, this post reviews the progress, tests Gemini 2.5 Pro, and reflects on the evolving potential and limitations of LLMs in cybersecurity."><link href=https://xvnpw.github.io/css/fonts.2c2227b81b1970a03e760aa2e6121cd01f87c88586803cbb282aa224720a765f.css rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://xvnpw.github.io/css/main.6a0f23ea50fd34b46fee262a5a68e17d458c51a2bc99ba1ba018065de6b180c3.css><link id=darkModeStyle rel=stylesheet type=text/css href=https://xvnpw.github.io/css/dark.50b57e12d401420df23965fed157368aba37b76df0ecefd0b1ecd4da664f01a0.css media="(prefers-color-scheme: dark)"><link rel=stylesheet type=text/css href=https://xvnpw.github.io/css/my.6e08ae20ebbcf10b8953bc0c3935a825ae172c99096f9af1bf2df91a6d21a1be.css></head><body><div class=content><header><div class=main><a href=https://xvnpw.github.io/>xvnpw personal blog</a></div><nav><a href=/>Home</a>
<a href=/posts>All posts</a>
<a href=/about>About</a>
<a href=/tags>Tags</a></nav></header><main><article><div class=post-container><div class=post-content><div class=title><h1 class=title>Threat Modeling with LLMs: Two Years In - Hype, Hope, and a Look at Gemini 2.5 Pro</h1><div class=meta>Posted on May 18, 2025</div></div><section class=body><p>It&rsquo;s been nearly two years since I began exploring how AI can enhance threat modeling processes. In this post, I&rsquo;ll share my latest findings with Gemini 2.5 Pro Preview, one of the newest advanced models, and reflect on how AI systems have evolved during this period. More importantly, I&rsquo;ll examine whether they&rsquo;re fulfilling their initial promise for security threat modeling applications.</p><figure class=image-center><img src=https://xvnpw.github.io/threat-modeling-with-llms-two-years-in-hype-hope-and-a-look-at-gemini-2.5-pro.png width=400></figure><h2 id=the-experiment-testing-gemini-25-pro>The Experiment: Testing Gemini 2.5 Pro</h2><p>As with my previous research, the goal is to assess how effectively LLMs can assist in threat modeling. For this, I used a deliberately underspecified architecture for a fictional project: &ldquo;AI Nutrition Pro&rdquo;.</p><figure class=image-center><img src=https://xvnpw.github.io/ai-nutrition-pro-arch.png width=auto></figure><p>You can find the <a href=https://github.com/xvnpw/ai-security-analyzer/blob/b7a9abde4f790da10c78f936d8e2161c125dfd4f/tests/EXAMPLE_ARCHITECTURE.md>architecture description here</a>. I intentionally left it lacking detail to see how well the LLM could handle ambiguity and missing information.</p><h3 id=methodology-ai-security-analyzer-and-prompting>Methodology: AI Security Analyzer and Prompting</h3><p>To conduct the threat modeling, I utilized my open-source tool, <a href=https://github.com/xvnpw/ai-security-analyzer>AI Security Analyzer</a>. The tool operates in <em>file</em> mode, reading the architecture description from a file and then running a specific prompt against the chosen LLM to generate a threat model in Markdown format.</p><p>The command looks like this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python ai_security_analyzer/app.py <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    file <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    -t tests/EXAMPLE_ARCHITECTURE.md <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    -o examples/threat-modeling.md <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --agent-model $agent_model <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --agent-temperature <span style=color:#e6db74>${</span>temperatures[$agent_model]<span style=color:#e6db74>}</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --agent-prompt-type threat-modeling <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --agent-provider $agent_provider
</span></span></code></pre></div><p>Check <a href=https://github.com/xvnpw/ai-security-analyzer/blob/main/scripts/create_examples.sh>create_examples.sh</a> for more details.</p><p>The prompt I used is based on the STRIDE per element methodology and guides the LLM through the necessary steps to create a comprehensive threat model. While it supports refinement iterations, this feature wasn&rsquo;t used in this specific experiment. You can view the <a href=https://github.com/xvnpw/ai-security-analyzer/blob/b7a9abde4f790da10c78f936d8e2161c125dfd4f/ai_security_analyzer/prompts/default/file/threat-modeling-default.yaml>full prompt structure here</a>.</p><p>A snippet of the prompt&rsquo;s core instruction:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span># IDENTITY and PURPOSE
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>You are an expert in risk and threat management and cybersecurity. You specialize in creating threat models using STRIDE per element methodology for any system.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># GOAL
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Given a FILE and CURRENT THREAT MODEL, provide a threat model using STRIDE per element methodology.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># STEPS
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> Think deeply about the input and what they are concerned with.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> Using your expertise, think about what they should be concerned with, even if they haven&#39;t mentioned it.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> Fully understand the STRIDE per element threat modeling approach.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> If CURRENT THREAT MODEL is not empty - it means that draft of this document was created in previous interactions with LLM using FILE content. In such case update CURRENT THREAT MODEL with new information that you get from FILE. In case CURRENT THREAT MODEL is empty it means that you first time get FILE content
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> Take the input provided and create a section called APPLICATION THREAT MODEL.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> Under that, create a section called ASSETS, take the input provided and determine what data or assets need protection. List and describe those.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>-</span> Under that, create a section called TRUST BOUNDARIES, identify and list all trust boundaries. Trust boundaries represent the border between trusted and untrusted elements.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>... (further steps omitted for brevity)
</span></span></code></pre></div><h3 id=a-note-on-limitations>A Note on Limitations</h3><p>A potential limitation is that all previous threat models from my experiments are publicly available in a GitHub repository. It&rsquo;s possible that the AI models I&rsquo;m testing have been trained on this data. In the future, I aim to prepare a new, unpublished dataset, perhaps inspired by projects like <a href=https://www.tmbench.com/>TM-Bench</a>, to mitigate this.</p><h3 id=results-gemini-25-pro-preview>Results: Gemini 2.5 Pro Preview</h3><p>I&rsquo;ll focus my comments on the &ldquo;Application Threat Model&rdquo; section generated by Gemini 2.5 Pro Preview.
You can find the <a href=https://github.com/xvnpw/ai-security-analyzer/blob/main/examples/file-threat-modeling-ai-nutrition-pro-gemini-2.5-pro-preview-05-06.md>full, detailed results here</a>.</p><p><strong>Application Threat Model Highlights (Gemini 2.5 Pro Preview):</strong></p><table><thead><tr><th>Item</th><th>Valid</th><th>Comment</th></tr></thead><tbody><tr><td>Assets</td><td>‚úÖ</td><td>Well-described and <em>completely valid</em> assets.</td></tr><tr><td>Trust Boundaries</td><td>‚ùå</td><td>While not entirely incorrect, I wouldn&rsquo;t typically define trust boundaries for internal services in this manner.</td></tr><tr><td>Data Flows</td><td>‚úÖ</td><td>Well-described and <em>completely valid</em> data flows.</td></tr><tr><td><strong>Threat:</strong> Stolen Meal Planner API key used to impersonate a legitimate client.</td><td>‚úÖ</td><td>Really like how it&rsquo;s described. Model mentioned what is currently implemented as mitigation and what is missing.</td></tr><tr><td><strong>Threat:</strong> Prompt injection attack modifies LLM behavior leading to unintended output or data leakage.</td><td>‚úÖ</td><td>Well-described and <em>completely valid</em> threat.</td></tr><tr><td><strong>Threat:</strong> Attacker bypasses ACL rules due to misconfiguration or overly permissive rules.</td><td>‚úÖ</td><td>Nicely captured possible misconfiguration (which can easily happened) and how it can be mitigated.</td></tr><tr><td><strong>Threat:</strong> Administrator credentials compromised, leading to unauthorized access to administrative functions.</td><td>‚úÖ</td><td>Correctly identified a critical threat. I omitted mitigations like MFA in the architecture to test the model&rsquo;s handling of such gaps.</td></tr><tr><td><strong>Threat:</strong> Unauthorized access to dietitian&rsquo;s content samples or LLM interaction logs via SQL injection in API Application.</td><td>‚úÖ</td><td>Some might argue SQL injection is less relevant for modern apps, but the architecture lacked explicit mitigations (e.g., SAST, DevSecOps practices). Ranked as medium.</td></tr><tr><td><strong>Threat:</strong> Excessive API calls to ChatGPT leading to high operational costs or rate limiting from OpenAI.</td><td>‚úÖ</td><td>Well-described and <em>completely valid</em> threat.</td></tr><tr><td><strong>Threat:</strong> Unauthorized access to tenant, billing, or configuration data via SQL injection in Web Control Plane.</td><td>‚úÖ</td><td>Similar reasoning as the API Application SQL injection threat; valid given the input.</td></tr><tr><td><strong>Threat:</strong> Sensitive data within prompts sent to ChatGPT is exposed due to ChatGPT&rsquo;s data handling or a breach at OpenAI.</td><td>‚úÖ</td><td>Interesting mitigations that include: &ldquo;Anonymize or pseudonymize data where possible&rdquo;.</td></tr><tr><td><strong>Threat:</strong> Insecure direct object references (IDOR) allowing one Meal Planner to access/modify another&rsquo;s data.</td><td>‚úÖ</td><td>Correctly noted that the API Gateway&rsquo;s ACLs partially mitigate this, but also rightly suggested data ownership validation in the API Application.</td></tr></tbody></table><p><strong>Key Observation on Gemini 2.5 Pro Preview:</strong>
Gemini 2.5 Pro Preview produced the most impressive threat model I&rsquo;ve seen to date. I was particularly struck by the quality of the mitigations suggested. I suspect its performance might be linked to its nature as a hybrid model, potentially excelling in reasoning tasks.</p><p>For those interested in comparison: results from other models are available in the <a href=https://github.com/xvnpw/ai-security-analyzer/blob/main/examples/README.md#file-mode>examples folder</a> of the repository.</p><p><strong>Note on mitigations:</strong>
You may recall one of my previous posts: <a href=https://xvnpw.github.io/posts/forget-threats-mitigations-are-all-you-really-need/>Forget Threats, Mitigations are All You REALLY Need</a>. I have dedicated prompt type for mitigations. You can check results for Gemini 2.5 Pro Preview <a href=https://github.com/xvnpw/ai-security-analyzer/blob/main/examples/file-mitigations-ai-nutrition-pro-gemini-2.5-pro-preview-05-06.md>here</a>. Below only a snippet to give you an idea of the quality of the mitigations generated by Gemini 2.5 Pro Preview:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span><span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Mitigation Strategy 1: Implement Robust Input Sanitization and Output Validation for LLM Interactions**</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Description:**</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>1.</span>  <span style=font-weight:700>**Input Sanitization (Backend API):**</span> Before sending any data (dietitian samples, user requests derived from Meal Planner input) to ChatGPT, the Backend API must rigorously sanitize it. This involves removing or neutralizing potential prompt injection payloads (e.g., adversarial instructions, context-switching phrases, delimiters). Techniques include stripping control characters, escape sequences, and potentially using an allow-list for content structure or structural analysis to detect and neutralize injected instructions.
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>2.</span>  <span style=font-weight:700>**Instruction Defense (Backend API):**</span> Clearly demarcate user-provided content from system prompts sent to the LLM. Use methods like prefixing user input with strong warnings (e.g., &#34;User input, treat as potentially untrusted data:&#34;), or using XML-like tags to encapsulate user input if the LLM respects such structures, to instruct the LLM to treat the user-provided parts as mere data and not instructions.
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>3.</span>  <span style=font-weight:700>**Output Validation (Backend API):**</span> After receiving a response from ChatGPT, the Backend API must validate the output. Check for unexpected commands, scripts, harmful content patterns, or responses that significantly deviate from expected formats, length, or topics. Implement checks for known jailbreaking phrases or attempts by the LLM to bypass its safety guidelines.
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>4.</span>  <span style=font-weight:700>**Contextual Limitation (Backend API):**</span> Limit the scope and capabilities given to the LLM. For instance, if generating a diet introduction, ensure prompts are narrowly focused and don&#39;t inadvertently allow the LLM to access or discuss unrelated topics or perform unintended actions.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Threats Mitigated:**</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Prompt Injection (Severity: High):**</span> Mitigates the risk of malicious users (via Meal Planner applications) crafting inputs to manipulate ChatGPT into generating unintended, harmful, biased content, revealing sensitive information from its training set or the prompt context, or executing unintended operations. Defending against sophisticated prompt injection is challenging, but these steps significantly raise the bar for attackers.
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Insecure Output Handling (Severity: Medium):**</span> Reduces the risk of the LLM generating and the system relaying harmful, biased, or nonsensical content by validating its output.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Impact:**</span> High. Significantly reduces the risk of LLM manipulation and the propagation of harmful content. While 100% prevention of prompt injection is difficult, these measures make successful attacks much harder and contain their impact.
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Currently Implemented:**</span> The API Gateway mentions &#34;filtering of input.&#34; However, this is likely generic input filtering (e.g., for XSS, SQLi at the gateway level) and not specialized for LLM prompt injection defense, which needs to occur closer to the LLM interaction point (Backend API).
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>*</span>   <span style=font-weight:700>**Missing Implementation:**</span> The Backend API requires specific, sophisticated input sanitization routines tailored for LLM prompts, instruction defense mechanisms, and output validation logic post-ChatGPT interaction.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>... (further mitigation strategies omitted for brevity)
</span></span></code></pre></div><h2 id=reassessing-ais-promise-in-threat-modeling>Reassessing AI&rsquo;s Promise in Threat Modeling</h2><p>When I began this research journey, I was genuinely excited about AI&rsquo;s potential to automate threat modeling. Two years later, I&rsquo;ve developed a more nuanced perspective on what&rsquo;s actually achievable.</p><h3 id=understanding-llm-capabilities-and-limitations>Understanding LLM Capabilities and Limitations</h3><p>Large Language Models use neural networks to process and generate human language with impressive results. Their ability to find solutions for problems that would be nearly impossible to solve with deterministic tools is remarkable.</p><p>However, can they truly replace human intelligence in security analysis? My research suggests not yet.</p><h3 id=comparison-with-traditional-automation-tools>Comparison with Traditional Automation Tools</h3><p>Traditional automation tools for threat modeling, such as <a href=https://github.com/OWASP/pytm>pytm</a>, typically rely on predefined rulesets and require users to describe their system architecture using a specific, formal syntax (e.g., <code>isPII=False</code>, <code>classification=Classification.PUBLIC</code>). These tools execute based on explicit programming: if certain conditions are met, specific threats are flagged.</p><p>LLMs present a clear contrast. Their ability to understand natural language means we can describe systems more intuitively, without being constrained by rigid syntaxes. This lowers the barrier to entry for threat modeling, making it more accessible to individuals who aren&rsquo;t deeply familiar with formal threat modeling methodologies or specialized tooling.</p><p>Furthermore, LLMs possess an advantage: they don&rsquo;t solely depend on pre-programmed threat libraries. Instead, they can reason from the provided system description and their training data to identify potential threats, even those not explicitly cataloged. Traditional tools, often limited by their configured rulesets (like <code>pytm</code> with its default set), may miss novel or context-specific threats that an LLM can infer based on broader patterns and knowledge.</p><p>In one of my early <a href=https://github.com/xvnpw/ai-nutrition-pro-design-claude3-opus/blob/main/ARCHITECTURE_SECURITY.md>experiments</a>, I explored a pipeline approach - breaking the process into smaller steps by first asking the LLM about threat modeling plan and data flows, then generating threats for each flow separately. This approach seems more promising for achieving consistent, high-quality results than single massive prompts.</p><h3 id=growing-capabilities-of-llms-since-gpt-35>Growing Capabilities of LLMs (since GPT-3.5)</h3><figure class=image-center><img src=https://xvnpw.github.io/llms-history.png width=auto></figure><p>This evolution has equipped LLMs with key capabilities crucial for threat modeling:</p><ul><li><strong>Enhanced Reasoning:</strong> Modern LLMs show an improved ability to follow complex instructions and apply conditional logic, parallel to a basic form of analytical thinking. This is evident in models that can expose their &rsquo;thought process&rsquo;, showing steps taken to meet criteria outlined in a prompt. For threat modeling, this translates to a better capacity to evaluate &lsquo;if-then&rsquo; scenarios and understand nuanced system interactions.</li><li><strong>Broad Knowledge Base:</strong> While technically represented as weights within a neural network, the extensive training data of LLMs effectively equips them with a vast knowledge base encompassing common software patterns, attack vectors, and security principles. This allows them to generate relevant information even with minimal context, as demonstrated in my <a href=https://github.com/xvnpw/sec-docs>sec-docs</a> project where LLMs produced security documentation for open-source software based solely on the project&rsquo;s name. In threat modeling, this broad knowledge enables them to suggest threats or consider attack vectors.</li></ul><h3 id=looking-forward>Looking Forward</h3><p>Models like Gemini 2.5 Pro demonstrate that current LLMs possess foundational capabilities that make them increasingly useful in the threat modeling lifecycle. Their flexibility in processing natural language and their embedded knowledge lower the barrier to initiating threat modeling activities.</p><p>Ultimately, the goal of threat modeling is not just to produce a document, but to drive concrete actions:</p><blockquote><p>The goal of threat modeling is to identify and mitigate threats to the system.</p></blockquote><p>In essence, we need to provide developers with actionable guidance on what to implement or fix. My past work on a GitHub Action generating <strong>security</strong> acceptance criteria for user stories (see <a href=https://github.com/xvnpw/ai-nutrition-pro-design-claude3-opus/issues/1>full example here</a>) hinted at this ideal.</p><figure class=image-center><img src=https://xvnpw.github.io/ac-opus.png width=auto></figure><p>But can current LLMs consistently deliver such actionable, context-aware security requirements? The path forward likely involves several interconnected developments:</p><ul><li><strong>Bigger and Better Models:</strong> The dream of a single, massive LLM that consumes an organization&rsquo;s entire context (code repositories, issue trackers, documentation) to provide consistently reliable security insights remains largely aspirational. My own experience, including recent tests with tools like Microsoft Copilot for enterprise search, indicates that even advanced models struggle with accuracy and consistency when faced with vast, diverse internal datasets.</li><li><strong>Specialized and Embedded Models:</strong> An alternative, perhaps more pragmatic, approach involves smaller, faster, and more specialized LLMs. These could excel at the decomposed prompting strategies mentioned earlier and become integral components of existing developer tools (e.g., Jira, Confluence, GitHub). Such models could continuously analyze specific contexts (like a user story or a code commit) and generate &lsquo;intermediate&rsquo; security-relevant data or suggestions, potentially fine-tuned on company-specific standards and previously identified issues to improve relevance.</li><li><strong>Multi-Agent Systems:</strong> The concept of multi-agent systems, where different AI agents specialize in sub-tasks (e.g., one agent analyzes code, another queries documentation, a third synthesizes findings), could enhance reliability, similar to how Chain-of-Thought prompting improves reasoning in single models. However, orchestrating these agents effectively and avoiding context overload for the synthesizing LLM (given its current reasoning limitations) remains a significant challenge. The risk is that an agent tasked with comprehensive information gathering might overwhelm the core LLM, diminishing the quality of the final threat model.</li></ul><p>However, even with advancements in these areas, a crucial gap will likely persist: the &ldquo;unwritten context&rdquo;. So much vital information for comprehensive threat modeling: business priorities, risk appetite, nuanced architectural decisions made in informal discussions, evolving team dynamics, etc. resides outside formal IT systems. Security professionals synthesize this by observing, discussing, and understanding the broader organizational landscape. Until we can effectively capture and integrate this vast, unstructured human context (from meetings, emails, informal chats), the human expert will remain indispensable in the threat modeling progress.</p><h3 id=refined-expectations-for-llms-in-threat-modeling-mid-2025>Refined Expectations for LLMs in Threat Modeling (Mid-2025)</h3><p>Based on my research, here are my current views on leveraging LLMs for threat modeling:</p><ul><li>‚≠êÔ∏è‚≠êÔ∏è <strong>Learning Aid</strong>: LLMs help flatten the learning curve for threat modeling newcomers.</li><li>‚≠êÔ∏è‚≠êÔ∏è <strong>Idea Generation</strong>: LLMs excel as inspiration tools. Developers can use them to create initial threat models before consulting security specialists. Matthew Adams highlights this approach in his <a href="https://www.youtube.com/watch?v=q5hMF46vgWE">STRIDE GPT presentation</a>.</li><li>‚≠êÔ∏è <strong>Rule-Based Alternative</strong>: LLMs might replicate deterministic tools like <code>pytm</code> when using a decomposed approach with smaller, focused prompts. Think of LLMs as having limited brain. They handle simple tasks reasonably well but struggle with complex reasoning that requires maintaining numerous dependencies. While they occasionally produce surprisingly good results (much like our pets can surprise us!), <strong>consistent performance</strong> remains elusive.</li><li>‚≠êÔ∏è‚≠êÔ∏è <strong>Context Enrichment</strong>: LLMs can identify threats and risks implied by context. It&rsquo;s something deterministic tools cannot do. This capability extends beyond threat modeling to vulnerability research, as discussed in my previous post: <a href=https://xvnpw.github.io/posts/can-ai-actually-find-real-security-bugs-testing-the-new-wave-of-ai-models/>Can AI Actually Find Real Security Bugs? Testing the New Wave of AI Models</a></li><li>‚≠êÔ∏è <strong>Domain-Specific Training</strong>: Training LLMs on architecture examples paired with expert-created threat models could improve their generalization capabilities. Threat modeling is a verifiable domain where we can score model performance and incorporate feedback into the learning process.</li><li>‚ùå <strong>Full Automation</strong>: I remain skeptical about using LLMs to completely automate threat modeling in a way it will be presented to developers as acceptance criteria. At least at this stage.</li></ul><h2 id=conclusion>Conclusion</h2><p>After reviewing dozens of AI-generated threat models over the past six months, I&rsquo;ve started to feel a bit of burnout üòÖ.</p><p>How do I balance my doubts with the strong results from Gemini 2.5 Pro Preview? While the model performed exceptionally well on my test case, I still wonder whether that performance will hold up across a wide range of real-world scenarios. A comprehensive benchmark - like the one mentioned earlier - could help answer that.</p><p>To be clear: LLMs <em>can</em> significantly enhance threat modeling workflows today. We don&rsquo;t need to wait for newer, more powerful models. What we do need is a solid understanding of their current limitations and a thoughtful approach to designing workflows around them.</p><p>If you&rsquo;re already successful using tools like <code>pytm</code> or other structured approaches to threat modeling, you&rsquo;ll likely find LLM-based automation helpful. But if you struggle with applying a rigid methodology, an LLM won&rsquo;t be a silver bullet.</p><hr><p>Thanks for reading! You can contact me and/or follow me on <a href=https://x.com/xvnpw>X</a> and <a href=https://linkedin.com/in/marcin-niemiec-304349104>LinkedIn</a>.</p></section><div class=post-tags><nav class="nav tags"><ul class=tags><li><a href=/tags/ai>ai</a></li><li><a href=/tags/llm>llm</a></li><li><a href=/tags/threat-modeling>threat modeling</a></li><li><a href=/tags/cybersecurity>cybersecurity</a></li><li><a href=/tags/chatgpt>chatgpt</a></li><li><a href=/tags/gemini>gemini</a></li></ul></nav></div></div></div></article></main><footer><div style=display:flex><a class=soc href=https://github.com/xvnpw rel=me title=GitHub><svg class="feather"><use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#github"/></svg></a><a class=border></a><a class=soc href=https://twitter.com/xvnpw rel=me title=Twitter><svg class="feather"><use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#twitter"/></svg></a><a class=border></a><a class=soc href=https://www.linkedin.com/in/marcin-niemiec-304349104/ rel=me title=Linkedin><svg class="feather"><use href="/svg/feather-sprite.51cf5647cb1987f769b616558f2620fd9423d72058490231b391bf6aa3744b55.svg#linkedin"/></svg></a><a class=border></a></div><div class=footer-info>2025 <a href=https://github.com/athul/archie>Archie Theme</a> | Built with <a href=https://gohugo.io>Hugo</a></div></footer><script async src="https://www.googletagmanager.com/gtag/js?id=G-YHETMXZXMZ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-YHETMXZXMZ")}</script></div></body></html>